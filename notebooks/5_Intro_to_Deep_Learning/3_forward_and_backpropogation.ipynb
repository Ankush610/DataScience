{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #2E86C1; font-weight: bold;\">Understanding Backpropagation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Most ML and AI libraries (e.g., TensorFlow, scikit-learn) allow you to implement backpropagation without needing to understand the math behind it.Understanding backpropagation enhances your ability to optimize neural networks and tackle complex deep learning architectures.\n",
    "\n",
    "## <span style=\"color: #D35400; font-weight: bold;\">What Is Backpropagation?</span>\n",
    "\n",
    "- **Iterative Process:**\n",
    "  - The machine learning process is iterative: we feed data into the model and evaluate its performance through an objective function.\n",
    "  - We adjust the model's parameters (weights and biases) using optimization algorithms to reach desired outputs.\n",
    "\n",
    "- **Forward Propagation:**\n",
    "  - Involves passing inputs through the network.\n",
    "  - At the end of each epoch, we compare the actual outputs to the target values to calculate errors.\n",
    "\n",
    "- **Backpropagation:**\n",
    "  - This process adjusts weights and biases in reverse based on calculated errors to minimize loss.\n",
    "  - Essential for improving the accuracy of artificial neural networks and a key part of the gradient descent optimization process.\n",
    "\n",
    "## <span style=\"color: #28B463; font-weight: bold;\">Components of a Deep Neural Network</span>\n",
    "\n",
    "- **Structure of the Network:**\n",
    "  - **Input Layer:** Contains two inputs, $ x_1 $ and $ x_2 $\n",
    "  - **Hidden Layer:** Comprises three hidden units (nodes), $ h_1, h_2, $ and $ h_3 $\n",
    "  - **Output Layer:** Produces two outputs, $ y_1 $ and $ y_2 $\n",
    "\n",
    "- **Weights:**\n",
    "  - Arrows connecting the layers represent weights.\n",
    "  - $ w $ weights connect the input and hidden layers.\n",
    "  - $ u $ weights connect the hidden and output layers.\n",
    "\n",
    "- **Targets:**\n",
    "  - The target values are denoted as $ t_1 $ and $ t_2 $.\n",
    "\n",
    "<center><img src=\"../../images/backpropogation_network.png\" alt=\"Deep Neural Network\" width=\"600\"/></center>\n",
    "\n",
    "## <span style=\"color: #F39C12; font-weight: bold;\">The Sigmoid Function</span>\n",
    "\n",
    "- **Non-Linearity:**\n",
    "  - Deep neural networks require non-linear activation functions to represent complex relationships.\n",
    "  - Stacking layers with only linear relationships is insufficient.\n",
    "\n",
    "- **Transformation:**\n",
    "  - Each arrow in the diagram represents a mathematical transformation of input values.\n",
    "  - Weights are applied to inputs, followed by the introduction of non-linearity, producing the hidden layer's units.\n",
    "\n",
    "- **Sigmoid Activation Function:**\n",
    "  - The sigmoid function is one of the most common activation functions:\n",
    "  \n",
    "  $$ \n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}} \n",
    "  $$\n",
    "\n",
    "  - Its derivative is:\n",
    "\n",
    "  $$ \n",
    "  \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \n",
    "  $$\n",
    "\n",
    "  - The sigmoid function modifies input values to produce outputs for the next layer.\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #2E86C1; font-weight: bold;\">The L2 Norm</span>\n",
    "\n",
    "- **Objective Functions:**\n",
    "  - Objective functions can be divided into loss (cost) functions and reward functions.\n",
    "  - This section focuses on loss functions, which measure prediction errors.\n",
    "\n",
    "- **Minimizing Cost:**\n",
    "  - A lower cost function indicates higher model accuracy.\n",
    "  - The goal is to minimize prediction error and, consequently, the cost.\n",
    "\n",
    "## <span style=\"color: #D35400; font-weight: bold;\">L2 Norm (Squared Loss)</span>\n",
    "\n",
    "- **Definition:**\n",
    "  - The L2 norm, commonly used in supervised learning and regression, represents a typical loss function.\n",
    "  - The term 'norm' refers to the Euclidean distance between the outputs and the targets.\n",
    "\n",
    "- **Calculation:**\n",
    "  - The L2-norm loss is computed by summing the squared differences between outputs $ y $ and targets $ t $.\n",
    "  - The mathematical expression is as follows:\n",
    "\n",
    "  $$\n",
    "  L = \\frac{1}{2} \\sum_{i} (y_i - t_i)^2\n",
    "  $$\n",
    "\n",
    "## <span style=\"color: #28B463; font-weight: bold;\">Backpropagation Algorithm</span>\n",
    "\n",
    "- **Separate Methodologies:**\n",
    "  - The backpropagation algorithm will be examined for both output and hidden layers.\n",
    "  - These methodologies differ, so they will be reviewed separately.\n",
    "\n",
    "- **Linear Model Function:**\n",
    "  - The linear model function is defined as:\n",
    "\n",
    "  $$\n",
    "  f(x) = xw + b\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $ x $ = input\n",
    "  - $ w $ = coefficient (weight)\n",
    "  - $ b $ = intercept (bias)\n",
    "\n",
    "- **Notation for Linear Combination:**\n",
    "  - For the linear combination before activation, we define:\n",
    "  \n",
    "  $$\n",
    "  a^{(1)} = xw + b^{(1)} \\quad \\text{and} \\quad a^{(2)} = hu + b^{(2)}\n",
    "  $$\n",
    "\n",
    "- **Output and Hidden Layer Activations:**\n",
    "  - Using this notation, the output $ y $ becomes the activated linear combination.\n",
    "  - For the output layer, we have:\n",
    "\n",
    "  $$\n",
    "  y = \\sigma(a^{(2)})\n",
    "  $$\n",
    "\n",
    "  - For the hidden layer, the activation is given by:\n",
    "\n",
    "  $$\n",
    "  h = \\sigma(a^{(1)})\n",
    "  $$\n",
    "\n",
    "## <span style=\"color: #F39C12; font-weight: bold;\">Common Functions</span>\n",
    "\n",
    "- **Focus on Activation and Loss Functions:**\n",
    "  - While there are various activation and loss functions, we concentrate on the most common ones:\n",
    "    - **Activation Function:** Sigmoid\n",
    "    - **Loss Function:** L2-norm loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #2E86C1; font-weight: bold;\">Backpropagation for the Output Layer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Objective of Backpropagation:**\n",
    "  - In supervised learning, the primary goal is to minimize the loss.\n",
    "  - Backpropagation computes the gradient of the loss function with respect to each unit's weights and biases in the network.\n",
    "\n",
    "- **Process Overview:**\n",
    "  - We use the gradients obtained to update parameters so that the loss computed with new values is less than the current loss.\n",
    "  - The loss decreases by iteratively adjusting the weights and biases based on the obtained gradients, allowing the network to gradually learn better predictions.\n",
    "\n",
    "## <span style=\"color: #D35400; font-weight: bold;\">Update Rule</span>\n",
    "\n",
    "- **Importance of Deltas:**\n",
    "  - Updates are directly related to the partial derivatives of the loss and indirectly connected to errors (deltas) between targets and outputs.\n",
    "  - Deltas enable modification of parameters using the update rule.\n",
    "\n",
    "- **Mathematical Expression:**\n",
    "  - The update rule for a weight $ u $ is given by:\n",
    "\n",
    "  $$\n",
    "  u \\leftarrow u - \\eta \\nabla_u L(u)\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $ \\eta $ (eta) is the learning rate of the machine learning algorithm.\n",
    "\n",
    "## <span style=\"color: #28B463; font-weight: bold;\">Calculating the Gradient</span>\n",
    "\n",
    "- **Single Weight Derivative:**\n",
    "  - For a single weight $ u_{ij} $, the partial derivative of the loss with respect to $ u_{ij} $ can be expressed as:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial u_{ij}} = \\frac{\\partial L}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial a^{(2)}_j} \\cdot \\frac{\\partial a^{(2)}_j}{\\partial u_{ij}}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $ i $ corresponds to the previous layer (input layer for this transformation).\n",
    "  - $ j $ corresponds to the next layer (output layer of the transformation).\n",
    "\n",
    "- **Using the Chain Rule:**\n",
    "  - The partial derivatives are computed using the chain rule:\n",
    "    1. **L2-Norm Loss Derivative:**\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial y_j} = (y_j - t_j)\n",
    "    $$\n",
    "\n",
    "    2. **Sigmoid Derivative:**\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial y_j}{\\partial a^{(2)}_j} = \\sigma(a^{(2)}_j)(1 - \\sigma(a^{(2)}_j)) = y_j(1 - y_j)\n",
    "    $$\n",
    "\n",
    "    3. **Derivative of the Linear Combination:**\n",
    "       - The linear model function is defined as:\n",
    "\n",
    "    $$\n",
    "    a^{(2)} = h u + b^{(2)}\n",
    "    $$\n",
    "\n",
    "       - The derivative of $ a^{(2)} $ with respect to $ u_{ij} $ is:\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial a^{(2)}_j}{\\partial u_{ij}} = h_i\n",
    "    $$\n",
    "\n",
    "## <span style=\"color: #F39C12; font-weight: bold;\">Final Expression</span>\n",
    "\n",
    "- **Substituting Partial Derivatives:**\n",
    "  - By combining these derivatives, we obtain:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial u_{ij}} = \\frac{\\partial L}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial a^{(2)}_j} \\cdot \\frac{\\partial a^{(2)}_j}{\\partial u_{ij}} = (y_j - t_j) \\cdot y_j(1 - y_j) \\cdot h_i = \\delta_j \\cdot h_i\n",
    "  $$\n",
    "\n",
    "- **Update Rule for the Output Layer:**\n",
    "  - Therefore, the update rule for a single weight in the output layer is:\n",
    "\n",
    "  $$\n",
    "  u_{ij} \\leftarrow u_{ij} - \\eta \\cdot \\delta_j \\cdot h_i\n",
    "  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #2E86C1; font-weight: bold;\">Backpropagation for a Hidden Layer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Updating Weights in Hidden Layers:**\n",
    "  - When dealing with deep neural networks, it's essential to update the weights across multiple hidden layers.\n",
    "  - The activation functions and their derivatives must also be considered when updating the weights.\n",
    "\n",
    "## <span style=\"color: #D35400; font-weight: bold;\">Update Rule for Hidden Layers</span>\n",
    "\n",
    "- **General Update Rule:**\n",
    "  - Similar to the output layer, the update rule for a single weight $ w_{ij} $ in the hidden layer can be expressed as:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial h_j} \\cdot \\frac{\\partial h_j}{\\partial a^{(1)}_j} \\cdot \\frac{\\partial a^{(1)}_j}{\\partial w_{ij}}\n",
    "  $$\n",
    "\n",
    "- **Calculating Partial Derivatives:**\n",
    "  - We compute backpropagation using the chain rule, utilizing the sigmoid activation and linear model formulas:\n",
    "\n",
    "  1. **Sigmoid Derivative:**\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial h_j}{\\partial a^{(1)}_j} = \\sigma(a^{(1)}_j)(1 - \\sigma(a^{(1)}_j)) = h_j(1 - h_j)\n",
    "  $$\n",
    "\n",
    "  2. **Linear Model Derivative:**\n",
    "\n",
    "  $$\n",
    "  a^{(1)} = x w + b^{(1)} \\quad \\Rightarrow \\quad \\frac{\\partial a^{(1)}_j}{\\partial w_{ij}} = x_i\n",
    "  $$\n",
    "\n",
    "## <span style=\"color: #28B463; font-weight: bold;\">Challenges in Calculating Derivatives</span>\n",
    "\n",
    "- **Hidden Layer Complexity:**\n",
    "  - Calculating $ \\frac{\\partial L}{\\partial h_j} $ is more complex as we don't have direct targets for the hidden layer outputs.\n",
    "  - Instead, we trace the contribution of each unit (hidden or otherwise) to the output errors.\n",
    "\n",
    "- **Example of Contribution Tracing:**\n",
    "  - For instance, consider a weight $ u_{11} $ that contributes to the output $ y_1 $ and its error $ e_1 $.\n",
    "  - The weight $ w_{11} $ contributes to $ h_1 $, which is connected to weights $ u_{11} $ and $ u_{12} $, impacting two outputs $ y_1 $ and $ y_2 $, and their respective errors $ e_1 $ and $ e_2 $.\n",
    "\n",
    "- **Backpropagating Errors:**\n",
    "  - To solve this, we backpropagate the errors through the network using the weights $ u $ to measure the contribution of the hidden layers to the errors, which aids in updating the weights $ w $.\n",
    "\n",
    "## <span style=\"color: #F39C12; font-weight: bold;\">Calculating Derivatives for Hidden Layers</span>\n",
    "\n",
    "- **Calculating $ \\frac{\\partial L}{\\partial h_1} $:**\n",
    "  - For weight $ w_{11} $, we calculate:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial a^{(2)}_1} \\cdot \\frac{\\partial a^{(2)}_1}{\\partial h_1} + \\frac{\\partial L}{\\partial y_2} \\cdot \\frac{\\partial y_2}{\\partial a^{(2)}_2} \\cdot \\frac{\\partial a^{(2)}_2}{\\partial h_1}\n",
    "  $$\n",
    "\n",
    "  - Substituting the derivatives, we have:\n",
    "\n",
    "  $$\n",
    "  = (y_1 - t_1) \\cdot y_1(1 - y_1) \\cdot u_{11} + (y_2 - t_2) \\cdot y_2(1 - y_2) \\cdot u_{12}\n",
    "  $$\n",
    "\n",
    "## <span style=\"color: #E74C3C; font-weight: bold;\">Final Expression for Weight Update</span>\n",
    "\n",
    "- **Calculating $ \\frac{\\partial L}{\\partial w_{11}} $:**\n",
    "  - The final expression for the weight update is:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial w_{11}} = \\left[ (y_1 - t_1) \\cdot y_1 \\cdot (1 - y_1) \\cdot u_{11} + (y_2 - t_2) \\cdot y_2 \\cdot (1 - y_2) \\cdot u_{12} \\right] \\cdot h_1(1 - h_1) \\cdot x_1\n",
    "  $$\n",
    "\n",
    "- **Generalized Form:**\n",
    "  - The generalized update rule for weights in hidden layers is given by:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial w_{ij}} = \\sum_k (y_k - t_k) \\cdot y_k(1 - y_k) \\cdot u_{jk} \\cdot h_j(1 - h_j) \\cdot x_i\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #28B463; font-weight: bold;\">How Does It Work? This Time In Simple Terms</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Forward Pass:**\n",
    "   - The model takes an input (like a question) and passes it through various layers (like chapters in a textbook) to get an output (an answer).\n",
    "\n",
    "2. **Calculating Error:**\n",
    "   - The model checks how far off its answer is from the correct answer. This difference is called the error.\n",
    "\n",
    "3. **Backward Pass:**\n",
    "   - Backpropagation then sends this error back through the network:\n",
    "     - It determines how much each weight (connection) contributed to the error.\n",
    "     - It uses this information to update the weights, making small adjustments to reduce the error in future predictions.\n",
    "\n",
    "4. **Learning Rate:**\n",
    "   - The learning rate is like how much you change your study methods based on feedback. A high learning rate means you make big changes, while a low learning rate means you make small changes.\n",
    "\n",
    "## <span style=\"color: #F39C12; font-weight: bold;\">Why is it Important?</span>\n",
    "\n",
    "- Backpropagation is crucial for training deep neural networks effectively.\n",
    "- It allows the model to learn complex patterns and make better predictions over time.\n",
    "\n",
    "## <span style=\"color: #E74C3C; font-weight: bold;\">In Summary</span>\n",
    "\n",
    "- **Think of Backpropagation as a Feedback Loop:** \n",
    "  - The model learns from its mistakes, adjusts its internal parameters, and improves its performance iteratively.\n",
    "- It helps machines get better at tasks, similar to how we learn from experience!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
