{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Optimizers</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:#2E86C1\"><b>What Are Optimizers?</b></span>\n",
    "\n",
    "Optimizers are essential algorithms in machine learning and deep learning, designed to adjust model parameters (weights and biases) to minimize the **loss function**, which measures how well the model is learning. Optimizers guide the training process, refining predictions by \"learning\" from data patterns. \n",
    "\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Why Do We Need Optimizers?</b></span>\n",
    "\n",
    "Optimizers are critical for efficient and effective training. Here’s why:\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Reducing Loss:</b></span> They minimize the loss, the difference between predicted and actual values, guiding models toward higher accuracy.\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Efficient Learning:</b></span> With deep learning’s vast parameter sets, optimizers provide a systematic way to adjust weights efficiently.\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Preventing Overfitting or Underfitting:</b></span> Optimizers fine-tune weights to avoid learning noise (overfitting) or missing patterns (underfitting).\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Handling Complex Error Surfaces:</b></span> In complex error surfaces, optimizers help navigate through local minima, saddle points, and flat regions to find optimal solutions.\n",
    "\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>How Optimizers Work</b></span>\n",
    "\n",
    "Optimizers adjust weights based on gradients derived from the **loss function**. Here’s a breakdown of the process:\n",
    "\n",
    "1. <span style=\"color:#9B59B6\"><b>Calculate the Gradient:</b></span> Each weight is evaluated to see how it influences the loss.\n",
    "  \n",
    "2. <span style=\"color:#9B59B6\"><b>Update the Weights:</b></span> Using the gradients, the optimizer adjusts the weights to decrease the loss.\n",
    "  \n",
    "3. <span style=\"color:#9B59B6\"><b>Repeat the Process:</b></span> This process continues iteratively, reducing the loss with each update until an acceptable level is reached.\n",
    "\n",
    "\n",
    "### <span style=\"color:#E74C3C\"><b>Key Takeaways</b></span>\n",
    "\n",
    "Optimizers are central to training as they navigate the loss landscape to improve accuracy. The right optimizer enhances stability, speed, and accuracy, ultimately determining how well a model performs on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../images/gradient_descent_visual.gif\" width=\"500\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1;\"><b>Batch, Stochastic, and Mini-Batch Gradient Descent</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:#D35400;\"><b>1. Batch Gradient Descent</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - Batch gradient descent computes the gradient of the loss function with respect to the entire dataset.\n",
    "  - At each iteration, all data points are processed simultaneously to calculate the gradient and adjust model parameters.\n",
    "  - This approach ensures that each update reflects the complete dataset, which usually leads to a smoother convergence path.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "    $$\n",
    "    w = w - \\alpha \\cdot \\text{Gradient}(w)\n",
    "    $$\n",
    "    - Here, **$ w $** is the weight, **$ \\alpha $** (alpha) is the learning rate, and **Gradient(w)** is the gradient of the loss function with respect to **$ w $**.\n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Stable Convergence**: Since it computes gradients over the full dataset, it produces stable, less noisy updates.\n",
    "  - **Fewer Oscillations**: Reduces random fluctuations as it reflects the global error gradient.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **High Computational Cost**: Processing the entire dataset per update can be very slow, especially for large datasets.\n",
    "  - **Memory Intensive**: Requires loading the entire dataset into memory, which can be problematic for big data applications.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Ideal for small-to-medium-sized datasets where memory and computational resources can handle the entire dataset at once.\n",
    "  - Useful when model training stability is prioritized over speed.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>2. Stochastic Gradient Descent (SGD)</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - In SGD, each model update is made after computing the gradient of the loss function with respect to just one data point.\n",
    "  - Each iteration uses a single randomly chosen data sample (stochastic means \"random\") to compute the gradient and adjust the parameters.\n",
    "  - This approach leads to frequent, smaller updates, introducing some randomness to the path of convergence.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "    $$\n",
    "    w = w - \\alpha \\cdot \\text{Gradient}(w; x^{(i)}, y^{(i)})\n",
    "    $$\n",
    "    - **$ x^{(i)} $** and **$ y^{(i)} $** represent a single input-output pair. This means we update the weight **$ w $** based on the gradient from each random sample.\n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Memory Efficient**: Requires only a single data point to compute each gradient, making it highly memory efficient.\n",
    "  - **Faster Iterations**: Can be faster for very large datasets as it updates frequently.\n",
    "  - **Explores Local Minima**: The randomness can help the model escape local minima, potentially finding better solutions.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **High Variance**: The updates are noisy, which can cause the model to converge slowly and even overshoot the minimum.\n",
    "  - **Unstable Convergence**: Without proper tuning, it can result in an erratic path to convergence, oscillating around the minimum.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Suitable for very large datasets, especially when computational resources are limited.\n",
    "  - Ideal for applications where quicker, approximate convergence is acceptable, like real-time systems or online learning.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>3. Mini-Batch Gradient Descent</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - Mini-Batch Gradient Descent splits the dataset into small, manageable batches (e.g., 32, 64, 128 samples per batch).\n",
    "  - The gradient is computed over each mini-batch, and model parameters are updated based on the average gradient of the samples in each mini-batch.\n",
    "  - This strikes a balance between the stability of Batch Gradient Descent and the speed of SGD.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "    $$\n",
    "    w = w - \\alpha \\cdot \\frac{1}{m} \\sum_{j=1}^{m} \\text{Gradient}(w; x^{(j)}, y^{(j)})\n",
    "    $$\n",
    "    - Here, **$ m $** is the batch size, and we take the average gradient over **$ m $** samples in the mini-batch. This gives us a middle ground between standard gradient descent and SGD. \n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Computational Efficiency**: Allows processing in smaller batches, which is faster and more memory-efficient than full-batch updates.\n",
    "  - **Reduced Noise**: Offers less noisy updates than SGD, helping to stabilize the convergence process.\n",
    "  - **Parallelization**: Mini-batches can be parallelized on hardware like GPUs, speeding up computations.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **Choice of Batch Size**: Choosing an optimal mini-batch size is important; a batch size too small may be noisy, while a large one may slow down training.\n",
    "  - **Memory Usage**: Larger mini-batches require more memory, which may limit batch size on certain hardware.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Commonly used for deep learning applications, especially when training on large datasets where full-batch processing is impractical.\n",
    "  - Preferred for tasks that benefit from both training stability and efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../images/different_optimizers.gif\" width=\"500\"></center>\n",
    "\n",
    "<center>\n",
    "<p>Animation of 5 gradient descent methods on a surface:\n",
    "<span style=\"color:cyan\">Gradient Descent (cyan)</span>, \n",
    "<span style=\"color:magenta\">Momentum (magenta)</span>, \n",
    "<span style=\"color:white; background-color:gray\">AdaGrad (white)</span>, \n",
    "<span style=\"color:green\">RMSProp (green)</span>, \n",
    "<span style=\"color:blue\">Adam (blue)</span>. \n",
    "Left well is the global minimum; right well is a local minimum.</p>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2E86C1;\"><b>Understanding Exponential Weighted Moving Average (EWMA)</b></span>\n",
    "\n",
    "In EWMA, we calculate a \"smoothed\" average of past values by giving more weight to recent values while exponentially decreasing the influence of older values. This technique helps reveal trends without being too \"jumpy.\"\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>Formula for EWMA</b></span>\n",
    "\n",
    "The EWMA formula is:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) x_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ v_t $: current value of the moving average,\n",
    "- $ \\beta $: smoothing parameter (between 0 and 1) controlling the weight of past values,\n",
    "- $ x_t $: latest data point.\n",
    "\n",
    "### <span style=\"color:#28B463;\"><b>The Role of $ \\beta $ and the \"Moody\" Analogy</b></span>\n",
    "\n",
    "- **High $ \\beta $ (e.g., 0.9)**: This setting makes the average \"less moody.\" The moving average is smoother and more stable, as it heavily considers past values, resulting in a gradual response to changes.\n",
    "  \n",
    "- **Low $ \\beta $ (e.g., 0.1)**: This setting makes the average \"moody\" or very sensitive to recent changes. It adapts quickly, prioritizing recent data points over past values.\n",
    "\n",
    "### <span style=\"color:#F39C12;\"><b>Example of EWMA with High and Low $ \\beta $</b></span>\n",
    "\n",
    "Consider a sequence of daily temperatures:\n",
    "\n",
    "| Day     | Temperature $ x_t $ |\n",
    "|---------|------------------------|\n",
    "| Day 1   | 20°C                  |\n",
    "| Day 2   | 22°C                  |\n",
    "| Day 3   | 21°C                  |\n",
    "| Day 4   | 24°C                  |\n",
    "\n",
    "Using an initial value of $ v_0 = 20 $, let’s examine how different values of $ \\beta $ affect the moving average.\n",
    "\n",
    "#### <span style=\"color:#E74C3C;\"><b>1. High $ \\beta = 0.9 $ (Less \"Moody\")</b></span>\n",
    "\n",
    "With $ \\beta = 0.9 $, each previous value has relatively high influence, resulting in a smooth trend.\n",
    "\n",
    "1. **Day 1**: $ v_1 = 0.9 \\times 20 + 0.1 \\times 20 = 20 $\n",
    "2. **Day 2**: $ v_2 = 0.9 \\times 20 + 0.1 \\times 22 = 20.2 $\n",
    "3. **Day 3**: $ v_3 = 0.9 \\times 20.2 + 0.1 \\times 21 = 20.3 $\n",
    "4. **Day 4**: $ v_4 = 0.9 \\times 20.3 + 0.1 \\times 24 = 20.67 $\n",
    "\n",
    "The values change slowly, with a stable trend.\n",
    "\n",
    "#### <span style=\"color:#9B59B6;\"><b>2. Low $ \\beta = 0.1 $ (More \"Moody\")</b></span>\n",
    "\n",
    "With $ \\beta = 0.1 $, the moving average quickly adapts to recent changes.\n",
    "\n",
    "1. **Day 1**: $ v_1 = 0.1 \\times 20 + 0.9 \\times 20 = 20 $\n",
    "2. **Day 2**: $ v_2 = 0.1 \\times 20 + 0.9 \\times 22 = 21.8 $\n",
    "3. **Day 3**: $ v_3 = 0.1 \\times 21.8 + 0.9 \\times 21 = 21.08 $\n",
    "4. **Day 4**: $ v_4 = 0.1 \\times 21.08 + 0.9 \\times 24 = 23.68 $\n",
    "\n",
    "With $ \\beta = 0.1 $, $ v_t $ changes quickly, adapting to the latest values, making it more sensitive or “moody.”\n",
    "\n",
    "### <span style=\"color:#2E86C1;\"><b>Why This Matters</b></span>\n",
    "\n",
    "- **High $ \\beta $**: Stable average; useful when quick fluctuations are less relevant, and you need a smoother trend.\n",
    "- **Low $ \\beta $**: Sensitive to recent data; ideal for capturing recent changes when the latest data is highly relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1;\"><b>Momentum Based Optimization and Impact of  Exponential Weighted Moving Average (EWMA) Formula Over Multiple Timesteps</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the EWMA formula over multiple timesteps shows how the importance (weight) of each previous value gradually decreases. This method reveals the diminishing effect of older data points, highlighting EWMA's focus on recent values.\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>Starting Formula</b></span>\n",
    "\n",
    "We begin with the standard EWMA formula:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) x_t\n",
    "$$\n",
    "\n",
    "Our goal is to expand $ v_{t-1} $ to demonstrate how past values contribute progressively smaller weights due to repeated multiplication by $ \\beta $.\n",
    "\n",
    "### <span style=\"color:#28B463;\"><b>Expanding $ v_t $ Across Multiple Timesteps</b></span>\n",
    "\n",
    "1. **Substitute $ v_{t-1} $**:\n",
    "   $$\n",
    "   v_t = \\beta \\left( \\beta v_{t-2} + (1 - \\beta) x_{t-1} \\right) + (1 - \\beta) x_t\n",
    "   $$\n",
    "   Expanding further:\n",
    "   $$\n",
    "   v_t = \\beta^2 v_{t-2} + \\beta (1 - \\beta) x_{t-1} + (1 - \\beta) x_t\n",
    "   $$\n",
    "\n",
    "2. **Substitute $ v_{t-2} $** similarly:\n",
    "   $$\n",
    "   v_t = \\beta^3 v_{t-3} + \\beta^2 (1 - \\beta) x_{t-2} + \\beta (1 - \\beta) x_{t-1} + (1 - \\beta) x_t\n",
    "   $$\n",
    "\n",
    "### <span style=\"color:#F39C12;\"><b>General Pattern</b></span>\n",
    "\n",
    "As we continue this process, a general pattern emerges for $ v_t $ across $ k $ timesteps:\n",
    "\n",
    "$$\n",
    "v_t = \\beta^k v_{t-k} + \\sum_{i=0}^{k-1} \\beta^i (1 - \\beta) x_{t-i}\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#E74C3C;\"><b>Key Observations</b></span>\n",
    "\n",
    "- **Repeated Multiplication by $ \\beta $**: Each previous term is multiplied by an additional factor of $ \\beta $.\n",
    "- **Weighting Past Terms**: The weight of each past value $ x_{t-i} $ is $ \\beta^i (1 - \\beta) $.\n",
    "- **Exponential Decay**: As $ i $ (the timestep difference) increases, $ \\beta^i $ becomes smaller because $ 0 < \\beta < 1 $. This means older values carry exponentially less weight over time.\n",
    "\n",
    "### <span style=\"color:#9B59B6;\"><b>Example with Specific $ \\beta $ Value</b></span>\n",
    "\n",
    "Let’s use $ \\beta = 0.9 $ to calculate the weights for recent timesteps:\n",
    "\n",
    "1. **Current timestep** $ x_t $: weight = $ (1 - 0.9) = 0.1 $\n",
    "2. **One timestep back** $ x_{t-1} $: weight = $ 0.9 \\times 0.1 = 0.09 $\n",
    "3. **Two timesteps back** $ x_{t-2} $: weight = $ 0.9^2 \\times 0.1 = 0.081 $\n",
    "4. **Three timesteps back** $ x_{t-3} $: weight = $ 0.9^3 \\times 0.1 = 0.0729 $\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2E86C1\"><b>Convex vs. Non-Convex Optimization</b></span>\n",
    "\n",
    "In optimization, understanding the shape of the function we are minimizing is essential. Here’s how convex and non-convex optimization differ:\n",
    "\n",
    "- **Convex Optimization**: In a convex landscape, there’s only one **global minimum**. This shape allows us to smoothly reach the lowest point without running into traps like local minima.\n",
    "- **Non-Convex Optimization**: In non-convex landscapes, there are **multiple local minima** and **saddle points**. This makes finding the global minimum more challenging, as gradients may lead us to local minima or saddle points instead.\n",
    "\n",
    "<center><img src=\"../../images/convex_non_convex.png\" alt=\"error\" width=\"600\"/></center>\n",
    "<center><p>Convex Gradient vs Non Convex Gradient</p></center>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Local Minima vs. Global Minima, Saddle Points, and High Curvature</b></span>\n",
    "\n",
    "- **Local Minimum**: A point where the function value is lower than in nearby points but may not be the lowest across the function.\n",
    "- **Global Minimum**: The lowest point across the entire function—ideal for the optimization process to find.\n",
    "- **Saddle Point**: A region where the gradient is zero but is neither a minimum nor a maximum. These points can slow down optimization as they often produce low gradients.\n",
    "- **High Curvature**: Areas of sharp changes in gradient cause unstable steps, potentially making the model overshoot the minimum or oscillate around it.\n",
    "\n",
    "<center><img src=\"../../images/local_global_minina_saddle_point.png\" alt=\"error\" width=\"600\"/></center>\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Constant Gradient</b></span>\n",
    "\n",
    "Occasionally, the gradient remains consistent regardless of position in the landscape. This can lead to overshooting if not managed carefully, as constant steps might not align with the function’s curvature.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#F39C12\"><b>Momentum-Based Optimization</b></span>\n",
    "\n",
    "Momentum-based optimization tackles challenges such as high curvature and saddle points by adding a **velocity term**. Here’s how it works:\n",
    "\n",
    "- **Adding a Velocity Term**: This smooths updates, so instead of updating weights based only on the current gradient, we incorporate **gradients from previous steps**. This \"builds speed\" in the direction where gradients align, enabling the model to move more swiftly.\n",
    "- **Navigating Complex Landscapes**: By integrating past gradients, the momentum term can move past saddle points or regions of high curvature, reducing the risk of stagnation.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#E74C3C\"><b>Mathematics of Momentum Optimization</b></span>\n",
    "\n",
    "The momentum update rule is:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla J(w)\n",
    "$$\n",
    "$$\n",
    "w = w - \\alpha v_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ v_t $: Velocity term, a weighted average of previous gradients.\n",
    "- $ \\beta $: Momentum parameter, which controls the influence of past gradients.\n",
    "- $ \\nabla J(w) $: Gradient of the cost function with respect to weights.\n",
    "- $ \\alpha $: Learning rate.\n",
    "\n",
    "The **velocity term** is calculated using *Exponential Weighted Moving Average (EWMA)* with the decay factor $\\beta$.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#9B59B6\"><b>Effect of the Decay Factor $ \\beta $</b></span>\n",
    "\n",
    "- **High $ \\beta $ (e.g., 0.9)**: A high $\\beta$ means more emphasis on past gradients, resulting in smoother, slower responses to current changes.\n",
    "- **Low $ \\beta $ (e.g., 0.1)**: A low $\\beta$ makes the model more responsive to recent changes, making it “moody” and more adaptable.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#2E86C1\"><b>Why $ \\beta $ Should Not Be 0 or 1</b></span>\n",
    "\n",
    "- **$ \\beta = 0 $**: This removes the velocity term, reducing momentum-based optimization to standard gradient descent, losing momentum benefits.\n",
    "- **$ \\beta = 1 $**: No decay in gradient influence leads to overshooting, as the same directional momentum is carried forward indefinitely, causing instability.\n",
    "\n",
    "Choosing an optimal $\\beta$ (between 0.8 and 0.99) allows the optimizer to balance historical gradients and recent changes, enabling efficient, steady progress through complex landscapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
