{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1;\"><b>Batch, Stochastic, and Mini-Batch Gradient Descent</b></span>\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>1. Batch Gradient Descent</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - Batch gradient descent computes the gradient of the loss function with respect to the entire dataset.\n",
    "  - At each iteration, all data points are processed simultaneously to calculate the gradient and adjust model parameters.\n",
    "  - This approach ensures that each update reflects the complete dataset, which usually leads to a smoother convergence path.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "  $$\n",
    "  w = w - \\alpha \\cdot \\nabla J(w)\n",
    "  $$\n",
    "  - Here, $ w $ represents the weight parameters, $ \\alpha $ is the learning rate, and $ \\nabla J(w) $ is the gradient of the loss over the entire dataset.\n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Stable Convergence**: Since it computes gradients over the full dataset, it produces stable, less noisy updates.\n",
    "  - **Fewer Oscillations**: Reduces random fluctuations as it reflects the global error gradient.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **High Computational Cost**: Processing the entire dataset per update can be very slow, especially for large datasets.\n",
    "  - **Memory Intensive**: Requires loading the entire dataset into memory, which can be problematic for big data applications.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Ideal for small-to-medium-sized datasets where memory and computational resources can handle the entire dataset at once.\n",
    "  - Useful when model training stability is prioritized over speed.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>2. Stochastic Gradient Descent (SGD)</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - In SGD, each model update is made after computing the gradient of the loss function with respect to just one data point.\n",
    "  - Each iteration uses a single randomly chosen data sample (stochastic means \"random\") to compute the gradient and adjust the parameters.\n",
    "  - This approach leads to frequent, smaller updates, introducing some randomness to the path of convergence.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "  $$\n",
    "  w = w - \\alpha \\cdot \\nabla J(w; x^{(i)}, y^{(i)})\n",
    "  $$\n",
    "  - Here, $ (x^{(i)}, y^{(i)}) $ is a single data point, and $ \\nabla J(w; x^{(i)}, y^{(i)}) $ represents the gradient of the loss with respect to that one data point.\n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Memory Efficient**: Requires only a single data point to compute each gradient, making it highly memory efficient.\n",
    "  - **Faster Iterations**: Can be faster for very large datasets as it updates frequently.\n",
    "  - **Explores Local Minima**: The randomness can help the model escape local minima, potentially finding better solutions.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **High Variance**: The updates are noisy, which can cause the model to converge slowly and even overshoot the minimum.\n",
    "  - **Unstable Convergence**: Without proper tuning, it can result in an erratic path to convergence, oscillating around the minimum.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Suitable for very large datasets, especially when computational resources are limited.\n",
    "  - Ideal for applications where quicker, approximate convergence is acceptable, like real-time systems or online learning.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>3. Mini-Batch Gradient Descent</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - Mini-Batch Gradient Descent splits the dataset into small, manageable batches (e.g., 32, 64, 128 samples per batch).\n",
    "  - The gradient is computed over each mini-batch, and model parameters are updated based on the average gradient of the samples in each mini-batch.\n",
    "  - This strikes a balance between the stability of Batch Gradient Descent and the speed of SGD.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "  $$\n",
    "  w = w - \\alpha \\cdot \\frac{1}{m} \\sum_{j=1}^{m} \\nabla J(w; x^{(j)}, y^{(j)})\n",
    "  $$\n",
    "  - Here, $ m $ is the mini-batch size, and the gradient is averaged across all samples within each mini-batch.\n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Computational Efficiency**: Allows processing in smaller batches, which is faster and more memory-efficient than full-batch updates.\n",
    "  - **Reduced Noise**: Offers less noisy updates than SGD, helping to stabilize the convergence process.\n",
    "  - **Parallelization**: Mini-batches can be parallelized on hardware like GPUs, speeding up computations.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **Choice of Batch Size**: Choosing an optimal mini-batch size is important; a batch size too small may be noisy, while a large one may slow down training.\n",
    "  - **Memory Usage**: Larger mini-batches require more memory, which may limit batch size on certain hardware.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Commonly used for deep learning applications, especially when training on large datasets where full-batch processing is impractical.\n",
    "  - Preferred for tasks that benefit from both training stability and efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
