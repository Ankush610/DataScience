{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1;\">**Popular Activation Functions**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **<span style=\"color:#2E86C1;\">What is an Activation Function?</span>**\n",
    "\n",
    "An **activation function** is a crucial mathematical function used in neural networks to decide whether a neuron should be activated (or \"fired\"). It introduces **non-linearity** into the model, enabling the network to learn and represent complex patterns, going beyond simple linear relationships. The choice of activation function impacts how well the model learns and generalizes on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## **<span style=\"color:#D35400;\">How Do Activation Functions Work?</span>**\n",
    "\n",
    "Activation functions are applied to the weighted sum of inputs that a neuron receives. This weighted sum is often referred to as **z**:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "Here, $ w_i $ represents the weights, $ x_i $ are the input features, and $ b $ is the bias term. The activation function $ f(z) $ transforms this sum into the neuron's output $ a $:\n",
    "\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "This transformation introduces **non-linearity**, enabling the neural network to capture complex relationships in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **<span style=\"color:#28B463;\">Why Use an Activation Function?</span>**\n",
    "\n",
    "Without activation functions, neural networks would behave like simple **linear models**. A series of linear transformations (e.g., weighted sums) cannot solve complex problems or learn non-linear relationships. Activation functions allow networks to:\n",
    "- **Model complex patterns** and learn from non-linear data.\n",
    "- **Control the signal flow**, ensuring only important features get passed through.\n",
    "- **Enable better learning** through deeper networks by avoiding linear collapse.\n",
    "\n",
    "**Key Purpose**: To break linearity and allow the neural network to model non-linear functions, which is essential for solving more complex tasks like image recognition, speech processing, and natural language understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## **<span style=\"color:#F39C12;\">Why Should an Activation Function Be Differentiable?</span>**\n",
    "\n",
    "Most neural networks are trained using the **backpropagation algorithm**, which requires the computation of the gradient (partial derivatives) of the loss function with respect to the weights. If the activation function isn't differentiable, the model cannot calculate gradients efficiently. \n",
    "\n",
    "**Why Differentiability Matters**:\n",
    "- Backpropagation relies on **gradient descent**, where the gradient of each layer is computed and propagated backward through the network.\n",
    "- The gradient helps update the weights and biases, guiding the network toward minimizing the error.\n",
    "\n",
    "---\n",
    "\n",
    "## **<span style=\"color:#E74C3C;\">Types of Activation Functions</span>**\n",
    "\n",
    "Activation functions can be broadly classified into two types:\n",
    "- **Linear Activation Functions**\n",
    "- **Non-Linear Activation Functions**\n",
    "\n",
    "Let's explore each type in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## **Linear Activation Function**\n",
    "\n",
    "A **linear activation function** has the form:\n",
    "\n",
    "$$\n",
    "f(x) = ax\n",
    "$$\n",
    "\n",
    "where $ a $ is a constant. It doesn’t introduce non-linearity into the model, as the output is directly proportional to the input.\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "f(x) = ax\n",
    "$$\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "(-∞, ∞)\n",
    "\n",
    "### **<span style=\"color:#28B463;\">Derivative</span>**:\n",
    "Constant $ a $, making it uninformative for weight updates during backpropagation.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros and Cons</span>**:\n",
    "- **Pros**: Simple computation, mathematically straightforward.\n",
    "- **Cons**: Lacks non-linearity, cannot solve problems with non-linear relationships, limited learning capability.\n",
    "\n",
    "### **Use Case**:\n",
    "- Mainly used in the **output layer** for regression tasks, where the output is a continuous value.\n",
    "\n",
    "---\n",
    "\n",
    "## **Non-Linear Activation Functions**\n",
    "\n",
    "Non-linear functions allow networks to model complex relationships. Here are the most widely used non-linear activation functions:\n",
    "\n",
    "## <span style=\"color:#F39C12;\">**Activation Function: Sigmoid**</span>\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "(0, 1)\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros</span>**\n",
    "- **Output bound**: Produces values between 0 and 1, making it ideal for probabilistic interpretations.\n",
    "- **Smooth gradient**: Enables optimization via gradient descent effectively due to its differentiability.\n",
    "- **Simple**: Easy to implement and understand.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Cons</span>**\n",
    "- **Vanishing gradients**: For large or small input values, the gradient approaches zero, slowing down learning (especially in deep networks).\n",
    "- **Non-zero-centered**: Outputs can be biased towards one class, affecting the convergence of the model.\n",
    "\n",
    "### **Use Case**: \n",
    "- **Binary classification**: Commonly used in the output layer of binary classification tasks.\n",
    "- **Logistic regression**: Utilized in models needing a probability estimate.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12;\">**Activation Function: Tanh**</span>\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "(-1, 1)\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros</span>**\n",
    "- **Zero-centered**: Outputs are centered around zero, which helps in faster convergence of the optimization algorithm.\n",
    "- **Strong gradient**: Derivative is steeper than sigmoid, allowing for stronger gradients during backpropagation.\n",
    "- **Smooth**: Continuous and differentiable, facilitating efficient training.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Cons</span>**\n",
    "- **Vanishing gradients**: Similar to sigmoid, it can also suffer from vanishing gradients for large positive or negative inputs, which can lead to slow learning rates in deep networks.\n",
    "- **Computationally expensive**: Involves exponential calculations, which may slow down training.\n",
    "\n",
    "### **Use Case**: \n",
    "- **Recurrent Neural Networks (RNNs)**: Preferred for hidden layers due to their zero-centered output and better convergence properties.\n",
    "- **General-purpose**: Useful in various neural network architectures.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12;\">**Activation Function: ReLU (Rectified Linear Unit)**</span>\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "[0, ∞)\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros</span>**\n",
    "- **Efficient**: Computationally inexpensive, as it requires only a thresholding at zero, which speeds up training.\n",
    "- **Sparse activation**: Only a portion of the neurons activates, leading to efficient representations.\n",
    "- **Mitigates vanishing gradients**: Maintains gradients for positive inputs, helping deep networks learn effectively.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Cons</span>**\n",
    "- **Dying ReLU problem**: Neurons can become inactive and only output zeros during training, especially when the learning rate is too high.\n",
    "- **Unbounded output**: Can lead to issues like exploding gradients during optimization, especially in deep networks.\n",
    "\n",
    "### **Use Case**: \n",
    "- **Deep Learning**: Commonly used in hidden layers of Convolutional Neural Networks (CNNs) and other deep learning architectures.\n",
    "- **Feedforward networks**: Frequently serves as a default activation function due to its benefits.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12;\">**Activation Function: Leaky ReLU**</span>\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\ \n",
    "\\alpha x & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$ \n",
    "(where $ \\alpha $ is a small constant)\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "(-∞, ∞)\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros</span>**\n",
    "- **Prevents dying ReLU**: Allows a small gradient when $ x < 0 $, which helps keep neurons active and learning.\n",
    "- **Retains benefits of ReLU**: Offers the same computational efficiency and sparse activation benefits as ReLU.\n",
    "- **More robust**: Better generalization due to non-zero outputs for negative inputs.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Cons</span>**\n",
    "- **More complexity**: Requires tuning the hyperparameter $ \\alpha $, which adds a layer of complexity in model training.\n",
    "- **Potentially slower convergence**: While it can prevent dying ReLUs, it may lead to slower convergence in certain scenarios.\n",
    "\n",
    "### **Use Case**: \n",
    "- **Deep Neural Networks**: Effective in deeper architectures where the dying ReLU problem is more pronounced.\n",
    "- **Convolutional Networks**: Common in CNNs and other deep models where neuron activation needs to be maintained.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12;\">**Activation Function: PReLU (Parametric ReLU)**</span>\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\ \n",
    "\\alpha x & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$ \n",
    "(where $ \\alpha $ is learnable)\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "(-∞, ∞)\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros</span>**\n",
    "- **Learnable parameters**: Allows the model to learn the value of $ \\alpha $, providing flexibility for each neuron during training.\n",
    "- **Mitigates dying ReLU**: Similar to Leaky ReLU, it prevents neurons from becoming inactive during training.\n",
    "- **Improved performance**: Empirically shown to lead to better model performance in various architectures.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Cons</span>**\n",
    "- **Increased complexity**: The introduction of a learnable parameter can complicate the model and may lead to overfitting.\n",
    "- **Slower training**: May increase training time due to additional parameters that need optimization.\n",
    "\n",
    "### **Use Case**: \n",
    "- **Deep Learning Applications**: Widely used in deep networks to improve performance, particularly in CNNs and GANs.\n",
    "- **Adaptive architectures**: Beneficial in architectures where flexibility in activation is crucial.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12;\">**Activation Function: ELU (Exponential Linear Unit)**</span>\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\ \n",
    "\\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "(-α, ∞)\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros</span>**\n",
    "- **Smooth transitions**: Provides a smooth output for negative values, which can lead to faster learning and better performance.\n",
    "- **Zero-centered**: Helps in reducing bias shifts and improves convergence speed.\n",
    "- **Non-saturating**: Avoids vanishing gradients by providing a non-zero gradient for negative inputs.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Cons</span>**\n",
    "- **Computational cost**: More expensive than ReLU due to the exponential function calculations.\n",
    "- **Hyperparameter tuning**: Requires tuning the parameter $ \\alpha $, which adds complexity to the training process.\n",
    "\n",
    "### **Use Case**: \n",
    "- **Deep Networks**: Particularly beneficial in deep networks where maintaining gradient flow is crucial.\n",
    "- **Convolutional Architectures**: Used in networks where faster convergence and learning speed are desired.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12;\">**Activation Function: GELU (Gaussian Error Linear Unit)**</span>\n",
    "\n",
    "### **<span style=\"color:#2E86C1;\">Formula</span>**:\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$ \n",
    "(where $ \\Phi(x) $ is the CDF of the Gaussian distribution)\n",
    "\n",
    "### **<span style=\"color:#D35400;\">Range</span>**:\n",
    "(-∞, ∞)\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Pros</span>**\n",
    "- **Probabilistic interpretation**: Weighs input by its likelihood of being active, introducing a stochastic element in activation.\n",
    "- **Smooth and differentiable**: Ensures that the function is smooth across its range, allowing for efficient optimization.\n",
    "- **Reduced risk of dying neurons**: Similar to PReLU and ELU, it avoids inactive neurons due to the smooth transition.\n",
    "\n",
    "### **<span style=\"color:#9B59B6;\">Cons</span>**\n",
    "- **Complexity**: The probabilistic nature makes it more complex to understand and implement than simpler functions like ReLU.\n",
    "- **Computationally intensive**: More expensive to compute due to the Gaussian function involved.\n",
    "\n",
    "### **Use Case**: \n",
    "- **Transformers and NLP models**: Commonly used in advanced architectures, especially in models like BERT and GPT for natural language processing tasks.\n",
    "- **Deep Learning Models**: Effective in deep networks needing a balance between linear and non-linear properties.\n",
    "\n",
    "\n",
    "## **<span style=\"color:#2E86C1;\">Choosing the Right Activation Function</span>**\n",
    "\n",
    "- **Sigmoid**: Best for **binary classification** problems.\n",
    "- **tanh**: Preferred when data is **zero-centered** or for **RNNs**.\n",
    "- **ReLU**: Default choice for most **hidden layers**.\n",
    "- **Leaky ReLU**: Use when facing the **dying ReLU problem**.\n",
    "- **PReLU**: Adds flexibility in learning from negative inputs.\n",
    "- **ELU**: Suitable for **deep networks** and fast convergence.\n",
    "- **GELU**: Used in **transformer models**.\n",
    "- **Swish**: Effective for **deep CNN architectures**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
