{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Optimizers</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:#2E86C1\"><b>What Are Optimizers?</b></span>\n",
    "\n",
    "Optimizers are essential algorithms in machine learning and deep learning, designed to adjust model parameters (weights and biases) to minimize the **loss function**, which measures how well the model is learning. Optimizers guide the training process, refining predictions by \"learning\" from data patterns. \n",
    "\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Why Do We Need Optimizers?</b></span>\n",
    "\n",
    "Optimizers are critical for efficient and effective training. Here’s why:\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Reducing Loss:</b></span> They minimize the loss, the difference between predicted and actual values, guiding models toward higher accuracy.\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Efficient Learning:</b></span> With deep learning’s vast parameter sets, optimizers provide a systematic way to adjust weights efficiently.\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Preventing Overfitting or Underfitting:</b></span> Optimizers fine-tune weights to avoid learning noise (overfitting) or missing patterns (underfitting).\n",
    "\n",
    "- <span style=\"color:#F39C12\"><b>Handling Complex Error Surfaces:</b></span> In complex error surfaces, optimizers help navigate through local minima, saddle points, and flat regions to find optimal solutions.\n",
    "\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>How Optimizers Work</b></span>\n",
    "\n",
    "Optimizers adjust weights based on gradients derived from the **loss function**. Here’s a breakdown of the process:\n",
    "\n",
    "1. <span style=\"color:#9B59B6\"><b>Calculate the Gradient:</b></span> Each weight is evaluated to see how it influences the loss.\n",
    "  \n",
    "2. <span style=\"color:#9B59B6\"><b>Update the Weights:</b></span> Using the gradients, the optimizer adjusts the weights to decrease the loss.\n",
    "  \n",
    "3. <span style=\"color:#9B59B6\"><b>Repeat the Process:</b></span> This process continues iteratively, reducing the loss with each update until an acceptable level is reached.\n",
    "\n",
    "\n",
    "### <span style=\"color:#E74C3C\"><b>Key Takeaways</b></span>\n",
    "\n",
    "Optimizers are central to training as they navigate the loss landscape to improve accuracy. The right optimizer enhances stability, speed, and accuracy, ultimately determining how well a model performs on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../images/gradient_descent_visual.gif\" width=\"500\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1;\"><b>Batch, Stochastic, and Mini-Batch Gradient Descent</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:#D35400;\"><b>1. Batch Gradient Descent</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - Batch gradient descent computes the gradient of the loss function with respect to the entire dataset.\n",
    "  - At each iteration, all data points are processed simultaneously to calculate the gradient and adjust model parameters.\n",
    "  - This approach ensures that each update reflects the complete dataset, which usually leads to a smoother convergence path.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "    $$\n",
    "    w = w - \\alpha \\cdot \\text{Gradient}(w)\n",
    "    $$\n",
    "    - Here, **$ w $** is the weight, **$ \\alpha $** (alpha) is the learning rate, and **Gradient(w)** is the gradient of the loss function with respect to **$ w $**.\n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Stable Convergence**: Since it computes gradients over the full dataset, it produces stable, less noisy updates.\n",
    "  - **Fewer Oscillations**: Reduces random fluctuations as it reflects the global error gradient.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **High Computational Cost**: Processing the entire dataset per update can be very slow, especially for large datasets.\n",
    "  - **Memory Intensive**: Requires loading the entire dataset into memory, which can be problematic for big data applications.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Ideal for small-to-medium-sized datasets where memory and computational resources can handle the entire dataset at once.\n",
    "  - Useful when model training stability is prioritized over speed.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>2. Stochastic Gradient Descent (SGD)</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - In SGD, each model update is made after computing the gradient of the loss function with respect to just one data point.\n",
    "  - Each iteration uses a single randomly chosen data sample (stochastic means \"random\") to compute the gradient and adjust the parameters.\n",
    "  - This approach leads to frequent, smaller updates, introducing some randomness to the path of convergence.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "    $$\n",
    "    w = w - \\alpha \\cdot \\text{Gradient}(w; x^{(i)}, y^{(i)})\n",
    "    $$\n",
    "    - **$ x^{(i)} $** and **$ y^{(i)} $** represent a single input-output pair. This means we update the weight **$ w $** based on the gradient from each random sample.\n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Memory Efficient**: Requires only a single data point to compute each gradient, making it highly memory efficient.\n",
    "  - **Faster Iterations**: Can be faster for very large datasets as it updates frequently.\n",
    "  - **Explores Local Minima**: The randomness can help the model escape local minima, potentially finding better solutions.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **High Variance**: The updates are noisy, which can cause the model to converge slowly and even overshoot the minimum.\n",
    "  - **Unstable Convergence**: Without proper tuning, it can result in an erratic path to convergence, oscillating around the minimum.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Suitable for very large datasets, especially when computational resources are limited.\n",
    "  - Ideal for applications where quicker, approximate convergence is acceptable, like real-time systems or online learning.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400;\"><b>3. Mini-Batch Gradient Descent</b></span>\n",
    "- **<span style=\"color:#28B463;\">Process</span>**:\n",
    "  - Mini-Batch Gradient Descent splits the dataset into small, manageable batches (e.g., 32, 64, 128 samples per batch).\n",
    "  - The gradient is computed over each mini-batch, and model parameters are updated based on the average gradient of the samples in each mini-batch.\n",
    "  - This strikes a balance between the stability of Batch Gradient Descent and the speed of SGD.\n",
    "- **<span style=\"color:#F39C12;\">Update Rule</span>**:\n",
    "    $$\n",
    "    w = w - \\alpha \\cdot \\frac{1}{m} \\sum_{j=1}^{m} \\text{Gradient}(w; x^{(j)}, y^{(j)})\n",
    "    $$\n",
    "    - Here, **$ m $** is the batch size, and we take the average gradient over **$ m $** samples in the mini-batch. This gives us a middle ground between standard gradient descent and SGD. \n",
    "- **<span style=\"color:#E74C3C;\">Pros</span>**:\n",
    "  - **Computational Efficiency**: Allows processing in smaller batches, which is faster and more memory-efficient than full-batch updates.\n",
    "  - **Reduced Noise**: Offers less noisy updates than SGD, helping to stabilize the convergence process.\n",
    "  - **Parallelization**: Mini-batches can be parallelized on hardware like GPUs, speeding up computations.\n",
    "- **<span style=\"color:#9B59B6;\">Cons</span>**:\n",
    "  - **Choice of Batch Size**: Choosing an optimal mini-batch size is important; a batch size too small may be noisy, while a large one may slow down training.\n",
    "  - **Memory Usage**: Larger mini-batches require more memory, which may limit batch size on certain hardware.\n",
    "- **<span style=\"color:#2E86C1;\">When to Use</span>**:\n",
    "  - Commonly used for deep learning applications, especially when training on large datasets where full-batch processing is impractical.\n",
    "  - Preferred for tasks that benefit from both training stability and efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../images/different_optimizers.gif\" width=\"500\"></center>\n",
    "\n",
    "<center>\n",
    "<p>Animation of 5 gradient descent methods on a surface:\n",
    "<span style=\"color:cyan\">Gradient Descent (cyan)</span>, \n",
    "<span style=\"color:magenta\">Momentum (magenta)</span>, \n",
    "<span style=\"color:white;\">AdaGrad (white)</span>, \n",
    "<span style=\"color:green\">RMSProp (green)</span>, \n",
    "<span style=\"color:blue\">Adam (blue)</span>. \n",
    "Left well is the global minimum; right well is a local minimum.</p>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
