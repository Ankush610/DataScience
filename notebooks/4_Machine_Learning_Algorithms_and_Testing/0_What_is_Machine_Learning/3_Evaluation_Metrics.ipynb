{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Model Evaluation Metrics</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics are used to assess how well a machine learning model performs on a given dataset. They help us understand how well the model can make predictions and highlight areas where the model may be biased or perform poorly.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>Why are Evaluation Metrics Important?</b></span>\n",
    "\n",
    "- **Assess Performance**: They quantify the quality of predictions made by the model.\n",
    "- **Highlight Strengths and Weaknesses**: Certain metrics reveal different aspects of the model's performance, such as how well it handles false positives or false negatives.\n",
    "- **Tune Models**: Metrics guide model tuning to improve results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Key Evaluation Metrics for Classification</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#D35400\"><b>1. Accuracy</b></span>\n",
    "\n",
    "- **Definition**: The ratio of correctly predicted instances to the total number of instances.\n",
    "    $$\n",
    "    \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}}\n",
    "    $$\n",
    "- **When to Use**: Use when the classes are balanced.\n",
    "- **Limitation**: Can be misleading with imbalanced data.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Precision</b></span>\n",
    "\n",
    "- **Definition**: The ratio of correctly predicted positive instances to the total predicted positives. It answers the question: *Of all the instances predicted as positive, how many were actually positive?*\n",
    "    $$\n",
    "    \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "    $$\n",
    "- **When to Use**: Use when minimizing false positives is important (e.g., spam detection).\n",
    "  \n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import precision_score\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Recall (Sensitivity or True Positive Rate)</b></span>\n",
    "\n",
    "- **Definition**: The ratio of correctly predicted positive instances to the total actual positives. It answers the question: *Of all the actual positive instances, how many were correctly identified?*\n",
    "    $$\n",
    "    \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "    $$\n",
    "- **When to Use**: Use when minimizing false negatives is important (e.g., disease detection).\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import recall_score\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. F1 Score</b></span>\n",
    "\n",
    "- **Definition**: The harmonic mean of Precision and Recall. It balances the trade-off between precision and recall, making it useful when you want a single measure of the model’s performance.\n",
    "    $$\n",
    "    F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "    $$\n",
    "- **When to Use**: Use when both precision and recall are equally important.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>5. ROC Curve (Receiver Operating Characteristic)</b></span>\n",
    "\n",
    "- **Definition**: The ROC curve is a plot of the True Positive Rate (Recall) against the False Positive Rate (FPR) at various threshold settings. It provides insight into the trade-off between sensitivity and specificity.\n",
    "  \n",
    "    - **True Positive Rate (Recall)**:\n",
    "    $$\n",
    "    \\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "    $$\n",
    "  \n",
    "    - **False Positive Rate**:\n",
    "    $$\n",
    "    \\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}\n",
    "    $$\n",
    "\n",
    "- **When to Use**: Use when the class distribution is imbalanced, and you want to evaluate the model's performance at different threshold levels.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>6. AUC-ROC (Area Under ROC Curve)</b></span>\n",
    "\n",
    "- **Definition**: AUC-ROC is a single value that summarizes the ROC curve, representing the likelihood that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance.\n",
    "  \n",
    "    - **When to Use**: Use when you need a performance metric for binary classification that balances both sensitivity and specificity.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>7. Confusion Matrix</b></span>\n",
    "\n",
    "- **Definition**: A table used to evaluate the performance of a classification model. It shows the count of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n",
    "\n",
    "    - **True Positive (TP)**: The model correctly predicted the positive class.\n",
    "    - **True Negative (TN)**: The model correctly predicted the negative class.\n",
    "    - **False Positive (FP)**: The model incorrectly predicted positive when it was actually negative.\n",
    "    - **False Negative (FN)**: The model incorrectly predicted negative when it was actually positive.\n",
    "  \n",
    "- **When to Use**: Use to get a detailed breakdown of how well the model is performing in terms of the types of correct and incorrect predictions it makes.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>8. Matthews Correlation Coefficient (MCC)</b></span>\n",
    "\n",
    "- **Definition**: MCC measures the correlation between the true and predicted binary classifications. It is considered a balanced measure even if the classes are of different sizes. It ranges from -1 (perfect disagreement) to +1 (perfect agreement).\n",
    "    $$\n",
    "    MCC = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "    $$\n",
    "- **When to Use**: Use when you want a single score that accounts for all four confusion matrix outcomes.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>9. Logarithmic Loss (Log Loss)</b></span>\n",
    "\n",
    "- **Definition**: Logarithmic Loss (Log Loss) penalizes incorrect classifications with probabilities that are far from the actual label. The goal is to minimize log loss, which gives better probability estimates.\n",
    "    $$\n",
    "    \\text{Log Loss} = - \\frac{1}{n} \\sum \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "    $$\n",
    "- **When to Use**: Use for evaluating probabilistic classifiers (e.g., logistic regression).\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import log_loss\n",
    "    log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Summary Table</b></span>\n",
    "\n",
    "| Metric                    | Definition                                                        | When to Use                                                |\n",
    "|---------------------------|-------------------------------------------------------------------|------------------------------------------------------------|\n",
    "| **Accuracy**               | Proportion of correctly classified instances                     | When classes are balanced                                  |\n",
    "| **Precision**              | Proportion of correctly predicted positives                      | When false positives are costly (e.g., spam detection)      |\n",
    "| **Recall (Sensitivity)**   | Proportion of actual positives correctly predicted               | When missing positives is costly (e.g., disease detection)  |\n",
    "| **F1 Score**               | Harmonic mean of precision and recall                            | When precision and recall are equally important             |\n",
    "| **ROC Curve**              | Plot of TPR vs FPR across different thresholds                   | When evaluating the model's trade-offs between sensitivity and specificity |\n",
    "| **AUC-ROC**                | Area under the ROC curve                                         | When you need a single performance measure for binary classification |\n",
    "| **Confusion Matrix**       | Breakdown of correct and incorrect predictions                   | When you want a detailed performance summary                |\n",
    "| **Matthews Correlation Coefficient (MCC)** | Balanced metric for binary classification      | When you want a single measure for imbalanced datasets      |\n",
    "| **Logarithmic Loss**       | Penalizes incorrect probabilities                                | When evaluating probabilistic models (e.g., logistic regression) |\n",
    "\n",
    "These metrics provide a comprehensive view of your model’s performance, helping you fine-tune it for optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Key Evaluation Metrics for  Regression</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression tasks involve predicting continuous values, and the key evaluation metrics focus on how far off the predictions are from the true values.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Mean Absolute Error (MAE)</b></span>\n",
    "\n",
    "- **Definition**: The average absolute difference between the predicted and actual values. It gives an idea of how wrong the predictions are in terms of units.\n",
    "    $$\n",
    "    MAE = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\n",
    "    $$\n",
    "- **When to Use**: Use when all errors are equally important.\n",
    "  \n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Mean Squared Error (MSE)</b></span>\n",
    "\n",
    "- **Definition**: The average of the squared differences between predicted and actual values. It penalizes larger errors more than smaller ones.\n",
    "    $$\n",
    "    MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "    $$\n",
    "- **When to Use**: Use when larger errors should be penalized more.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Root Mean Squared Error (RMSE)</b></span>\n",
    "\n",
    "- **Definition**: The square root of the MSE, which provides error estimates in the same units as the target variable.\n",
    "    $$\n",
    "    RMSE = \\sqrt{MSE}\n",
    "    $$\n",
    "- **When to Use**: Use when you want to interpret errors in terms of actual units, and larger errors are more heavily penalized.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. R-Squared (Coefficient of Determination)</b></span>\n",
    "\n",
    "- **Definition**: Measures the proportion of the variance in the target variable that is predictable from the independent variables. It ranges from 0 to 1, with 1 indicating perfect predictions.\n",
    "    $$\n",
    "    R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "    $$\n",
    "- **When to Use**: Use to assess how well your model explains the variance in the target variable.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>5. Adjusted R-Squared</b></span>\n",
    "\n",
    "- **Definition**: A modified version of R-Squared that adjusts for the number of predictors in the model. It accounts for the fact that R-Squared always increases as more predictors are added, even if they don't improve the model.\n",
    "    $$\n",
    "    \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
    "    $$\n",
    "- **When to Use**: Use when you are comparing models with different numbers of predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>6. Mean Absolute Percentage Error (MAPE)</b></span>\n",
    "\n",
    "- **Definition**: The average of the absolute percentage differences between predicted and actual values. It expresses errors as a percentage of the true values.\n",
    "    $$\n",
    "    MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\n",
    "    $$\n",
    "- **When to Use**: Use when you want to express error as a percentage.\n",
    "\n",
    "    - **Example Code**:\n",
    "    ```bash\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#2E86C1\"><b>Summary of Regression Metrics</b></span>\n",
    "\n",
    "| Metric                     | Definition                                                         | When to Use                                                |\n",
    "|----------------------------|--------------------------------------------------------------------|------------------------------------------------------------|\n",
    "| **MAE**                     | Average absolute difference between predicted and actual values    | When all errors are equally important                      |\n",
    "| **MSE**                     | Average squared difference between predicted and actual values     | When larger errors should be penalized more                 |\n",
    "| **RMSE**                    | Square root of the MSE, giving error in the same units as target   | When interpreting errors in actual units                   |\n",
    "| **R-Squared**               | Proportion of variance explained by the model                     | When assessing how well the model explains target variance  |\n",
    "| **Adjusted R-Squared**      | R-Squared adjusted for the number of predictors                   | When comparing models with different numbers of predictors  |\n",
    "| **MAPE**                    | Average absolute percentage difference                            | When expressing errors as percentages                       |\n",
    "\n",
    "These regression metrics provide a comprehensive view of model performance by focusing on prediction errors and how well the model explains the variance in the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
