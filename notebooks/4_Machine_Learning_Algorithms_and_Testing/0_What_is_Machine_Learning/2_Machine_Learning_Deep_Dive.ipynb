{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Linear Regression Model: A Deep Dive</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear regression is a fundamental algorithm used in both **machine learning (ML)** and **deep learning (DL)**. It models the relationship between a dependent variable \\($ y $\\) and an independent variable \\($ x $\\) by fitting a **linear equation**. Let's break down the model step by step.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Basic Model Architecture</b></span>\n",
    "\n",
    "In a **deep learning** context, linear regression can be viewed as a **simple neural network** with:\n",
    "\n",
    "- **One input** (the feature \\($ x $\\)),\n",
    "- **One neuron** (which applies a linear transformation \\($ y = wx + b $\\)),\n",
    "- **One output** (the predicted value \\($ \\hat{y} $\\)).\n",
    "\n",
    "The equation for linear regression is:\n",
    "\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ w $\\) is the **weight** (or slope of the line),\n",
    "- \\($ b $\\) is the **bias** (the y-intercept of the line),\n",
    "- \\($ x $\\) is the **input feature**,\n",
    "- \\($ \\hat{y} $\\) is the **predicted output**.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Loss Function</b></span>\n",
    "\n",
    "The goal of linear regression is to minimize the error between the predicted values \\($ \\hat{y} $\\) and the actual values \\($ y $\\). This error is quantified using a **loss function**. The most common loss function for linear regression is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ n $\\) is the **number of data points**,\n",
    "- \\($ y_i $\\) is the **actual value** of the \\($ i $\\)-th data point,\n",
    "- \\($ \\hat{y}_i $\\) is the **predicted value**.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Gradient Descent (Weight and Bias Updates)</b></span>\n",
    "\n",
    "To minimize the loss function, we use **gradient descent**. This optimization algorithm updates the weights and biases by computing the **gradients** of the loss function with respect to these parameters.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Understanding Gradient Descent</b></span>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. What is a Gradient?</b></span>\n",
    "\n",
    "- The **gradient** is a vector that points in the direction of the **steepest ascent** of a function.\n",
    "- It indicates how much the function will increase (or decrease) if you move in that direction.\n",
    "- In simple terms, it gives you the **slope** of the function at a specific point.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. What is Gradient Descent?</b></span>\n",
    "\n",
    "- **Gradient Descent** is an optimization algorithm used to minimize a function (often a loss function in machine learning).\n",
    "- It involves taking **steps downhill** towards the minimum point of the function.\n",
    "- The algorithm uses the gradient to determine the direction to move and how far to go.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. What is a Partial Derivative?</b></span>\n",
    "\n",
    "- A **partial derivative** measures how a function changes as one of its variables changes while keeping other variables constant.\n",
    "- It allows us to understand the **sensitivity** of the function to each parameter independently.\n",
    "- In the context of gradient descent, partial derivatives are used to calculate the gradient.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>4. Key Concepts</b></span>\n",
    "\n",
    "- **Steepest Ascent vs. Steepest Descent**:\n",
    "  - The **gradient** indicates the direction of steepest ascent.\n",
    "  - **Gradient Descent** uses the negative gradient to find the direction of steepest descent.\n",
    "  \n",
    "- **Learning Rate**:\n",
    "  - The **learning rate** \\( $\\alpha$ \\) controls the size of the steps taken during the descent.\n",
    "  - A small learning rate leads to slow convergence, while a large learning rate may overshoot the minimum.\n",
    "\n",
    "- **Iteration**:\n",
    "  - Gradient descent is performed iteratively, updating the weights or parameters until convergence (when the updates become negligibly small).\n",
    "\n",
    "<center><img src=\"../../../images/gradient_descent.png\" alt=\"error\" width=\"800\"/></center>\n",
    "---\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Gradient of the Loss Function</b></span>\n",
    "\n",
    "We compute the **partial derivatives** of the loss function with respect to both \\($ w $\\) (weight) and \\($ b $\\) (bias):\n",
    "\n",
    "- **Partial derivative w.r.t \\($ w $\\)**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Partial derivative w.r.t \\($ b $\\)**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Updating Weight and Bias</b></span>\n",
    "\n",
    "The **gradient descent** algorithm updates \\($ w $\\) and \\($ b $\\) iteratively using the **learning rate** \\($ \\alpha $\\) (which controls the step size of the update):\n",
    "\n",
    "- **Weight update**:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "- **Bias update**:\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "These updates are performed over multiple **epochs** (iterations over the entire dataset) until the loss converges to a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Key Terms Explained</b></span>\n",
    "\n",
    "- **Loss**: This is a measure of how far off the predicted values \\($ \\hat{y} $\\) are from the actual values \\($ y $\\). In linear regression, the loss is often calculated using the **MSE**.\n",
    "\n",
    "- **Gradient Descent**: This is the optimization algorithm used to minimize the loss function by adjusting the model's parameters (**weight** and **bias**). It works by moving in the direction of the steepest decrease of the loss (negative gradient).\n",
    "\n",
    "- **Epoch**: An epoch refers to one complete pass through the entire training dataset. In each epoch, the model updates its weights and biases using gradient descent.\n",
    "\n",
    "- **Weight (w)**: This is the parameter that determines how much influence the input variable \\($ x $\\) has on the output \\($ y $\\). It's equivalent to the slope of the line in linear regression.\n",
    "\n",
    "- **Bias (b)**: This is the parameter that shifts the line up or down. It's equivalent to the y-intercept in linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Calculating the Updated Weight (w)</b></span>\n",
    "\n",
    "Let's now dive deeper into the **equation** to calculate the updated **weight** \\($ w $\\). We will substitute values step by step to get a final formula.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Starting Point</b></span>\n",
    "\n",
    "We are using **gradient descent** to update \\($ w $\\):\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\alpha $\\) is the **learning rate**,\n",
    "- \\($ \\frac{\\partial \\text{Loss}}{\\partial w} $\\) is the **gradient of the loss** with respect to \\($ w $\\).\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Loss Function and Gradient</b></span>\n",
    "\n",
    "For linear regression, the predicted value \\($ \\hat{y} $\\) is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "And the loss is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Substitute \\($ \\hat{y}_i = w \\cdot x_i + b $\\) into the loss function:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b))^2\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Gradient Calculation</b></span>\n",
    "\n",
    "Take the partial derivative of the loss function with respect to \\($ w $\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - (w \\cdot x_i + b))\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Bias Substitution \\( b = $\\bar{y}$ - w $\\cdot$ $\\bar{x}$ \\)</b></span>\n",
    "\n",
    "Now, substitute the bias term \\($ b = \\bar{y} - w \\cdot \\bar{x} $\\), where:\n",
    "\n",
    "- \\( $\\bar{y}$ \\) is the **mean** of \\($ y $\\),\n",
    "- \\( $\\bar{x}$ \\) is the **mean** of \\($ x $\\).\n",
    "\n",
    "Thus, the predicted value becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w \\cdot (x - \\bar{x}) + \\bar{y}\n",
    "$$\n",
    "\n",
    "Substitute this back into the **gradient** of the loss:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i \\left( (y_i - \\bar{y}) - w \\cdot (x_i - \\bar{x}) \\right)\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Final Equation for Weight Update</b></span>\n",
    "\n",
    "Finally, the **weight** gets updated using **gradient descent**:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i \\left( (y_i - \\bar{y}) - w_{\\text{old}} \\cdot (x_i - \\bar{x}) \\right)\n",
    "$$\n",
    "\n",
    "This formula shows how the weight \\($ w $\\) gets updated in each step of the gradient descent process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Ridge Regression: Introducing Regularization</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Ridge Regression**, the primary difference from ordinary linear regression is the inclusion of a **regularization term**. This term helps penalize large weights to prevent **overfitting**, leading to a model that generalizes better.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Ridge Regression Loss Function</b></span>\n",
    "\n",
    "The **Ridge Regression Loss** function combines the **Mean Squared Error (MSE)** with a penalty term that controls the magnitude of the weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Ridge}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda w^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( $\\lambda$ \\) is the **regularization parameter** (often referred to as **alpha** in Ridge, which should not be confused with the learning rate \\( $\\alpha$ \\)),\n",
    "- \\( $w^2$ \\) is the **sum of the squared weights**.\n",
    "\n",
    "The additional term \\( $\\lambda w^2$ \\) discourages the model from learning large weights, which may lead to overfitting.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Ridge Loss with Respect to Weight \\( $w$ \\)</b></span>\n",
    "\n",
    "To derive the weight update rule, we need to compute the **gradient of the Ridge Loss** with respect to the weight \\( $w$ \\). This consists of two parts:\n",
    "\n",
    "- **Gradient of the MSE** (same as in linear regression):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the regularization term \\( \\lambda w^2 \\)**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda w^2}{\\partial w} = 2\\lambda w\n",
    "$$\n",
    "\n",
    "Thus, the total gradient for **Ridge Regression** becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Ridge}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + 2\\lambda w\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Ridge Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\( w \\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + 2\\lambda w_{\\text{old}} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying the update formula:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - 2\\alpha\\lambda w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "This final equation shows how the weight is updated at each step of gradient descent in **Ridge Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Key Differences from Ordinary Linear Regression</b></span>\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Regularization Term</b></span>\n",
    "\n",
    "The key difference in Ridge Regression is the inclusion of the **second term** \\( $2\\alpha\\lambda w_{\\text{old}}$ \\) in the weight update rule. This term penalizes large values of \\( $w$ \\), shrinking the weights over time and helping to prevent **overfitting**.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Regularization Parameter \\( $\\lambda$ \\)</b></span>\n",
    "\n",
    "- The **regularization parameter \\( $\\lambda$ \\)** controls the strength of the penalty. \n",
    "- When \\( $\\lambda$ = 0 \\), Ridge Regression becomes equivalent to **ordinary linear regression**. \n",
    "- A larger \\( $\\lambda$ \\) results in greater penalization, pushing the weights towards zero and reducing model complexity.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact on Generalization</b></span>\n",
    "\n",
    "The regularization term \\( $\\lambda w^2$ \\) encourages the model to have **smaller weights**, preventing it from overfitting the training data. This allows the model to generalize better to unseen data, avoiding **overly complex solutions** that fit noise in the data.\n",
    "\n",
    "---\n",
    "\n",
    "By adding **Ridge regularization**, we improve the **stability** of the linear model, especially when dealing with **multicollinearity** (where predictor variables are highly correlated). Ridge Regression is an effective tool when you need to balance between fitting your data and maintaining a model that generalizes well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Lasso Regression: Emphasizing Feature Selection</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Lasso Regression**, the key difference from ordinary linear regression is the introduction of a **regularization term** that encourages sparsity in the model. This means that some coefficients can become exactly zero, leading to a simpler and more interpretable model.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Lasso Regression Loss Function</b></span>\n",
    "\n",
    "The **Lasso Regression Loss** function integrates the **Mean Squared Error (MSE)** with a penalty term based on the absolute values of the weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Lasso}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |w_j|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\lambda $\\) is the **regularization parameter** (also called **alpha** in Lasso, which should not be confused with the learning rate \\($ \\alpha $\\)),\n",
    "- \\($ |w_j| $\\) is the **absolute sum of the weights**.\n",
    "\n",
    "The term \\($ \\lambda \\sum_{j=1}^{p} |w_j| $\\) encourages some weights to shrink to zero, effectively performing feature selection.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Lasso Loss with Respect to Weight \\($ w $\\)</b></span>\n",
    "\n",
    "To derive the weight update rule for Lasso Regression, we compute the **gradient of the Lasso Loss** with respect to the weight \\($ w $\\). The gradient consists of two components:\n",
    "\n",
    "- **Gradient of the MSE** (same as in linear regression):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the regularization term \\($ \\lambda |w| $\\)**:\n",
    "\n",
    "The derivative with respect to \\($ w $\\) involves the **sign function**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda |w|}{\\partial w} = \\lambda \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "So, the total gradient for **Lasso Regression** is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Lasso}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Lasso Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\($ w $\\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) \\right)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - \\alpha \\lambda \\cdot \\text{sgn}(w_{\\text{old}})\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. Key Differences from Ordinary Linear Regression</b></span>\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Regularization Term</b></span>\n",
    "\n",
    "The major distinction in Lasso Regression is the inclusion of the **absolute value term** \\( \\alpha \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) \\) in the weight update rule. This term can drive some weights exactly to zero, allowing the model to exclude less important features.\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Regularization Parameter \\( \\lambda \\)</b></span>\n",
    "\n",
    "- The **regularization parameter \\($ \\lambda $\\)** controls the strength of the penalty.\n",
    "- When \\($ \\lambda = 0 $\\), Lasso Regression becomes equivalent to **ordinary linear regression**.\n",
    "- A larger \\($ \\lambda $\\) increases the penalty, promoting more weights to become zero and leading to a simpler model.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Weight Updates in Lasso Regression</b></span>\n",
    "\n",
    "In Lasso Regression, the weight update rule incorporates the term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\). This term plays a critical role in how the weights are adjusted during training. Let's break down its impact:\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding the Penalty Term</b></span>\n",
    "\n",
    "- **\\($ \\lambda $\\)**: This is the regularization parameter that controls the strength of the penalty. A larger \\($ \\lambda $\\) encourages more weights to shrink towards zero.\n",
    "  \n",
    "- **\\($ \\text{sgn}(w_{\\text{old}})$\\)**: The sign function returns:\n",
    "  - **1** if \\($ w_{\\text{old}} > 0 $\\) (positive weight)\n",
    "  - **-1** if \\($ w_{\\text{old}} < 0 $\\) (negative weight)\n",
    "  - **0** if \\($ w_{\\text{old}} = 0 $\\) (zero weight)\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact of the Penalty Term</b></span>\n",
    "\n",
    "1. **When \\($ w_{\\text{old}} $\\) is Positive**:\n",
    "   - The term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\) contributes positively to the weight update.\n",
    "   - **Effect**: The penalty reduces the value of the weight \\($ w_{\\text{new}} $\\). \n",
    "   - **Interpretation**: This encourages the weight to shrink, thus regularizing the model.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left(\\text{penalty}\\right) \\quad \\text{(penalty is positive)}\n",
    "   $$\n",
    "\n",
    "2. **When \\($ w_{\\text{old}} $\\) is Negative**:\n",
    "   - The term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\) contributes negatively to the weight update.\n",
    "   - **Effect**: The penalty increases the value of the weight \\($ w_{\\text{new}} $\\) (making it less negative).\n",
    "   - **Interpretation**: This adjustment reduces the magnitude of the negative weight, pushing it closer to zero.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left(\\text{penalty}\\right) \\quad \\text{(penalty is negative)}\n",
    "   $$\n",
    "\n",
    "3. **When \\($ w_{\\text{old}} $\\) is Small (Close to Zero)**:\n",
    "   - If \\($ |w_{\\text{old}}| $\\) is small enough, the penalty can effectively drive the weight to exactly zero.\n",
    "   - **Effect**: The weight \\($ w_{\\text{new}} $\\) becomes zero, effectively eliminating that feature from the model.\n",
    "   - **Interpretation**: This feature selection property is a key benefit of Lasso Regression.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = 0 \\quad \\text{(if the update drives \\( w_{\\text{old}} \\) to zero)}\n",
    "   $$\n",
    "\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Impact on Feature Selection</b></span>\n",
    "\n",
    "The L1 penalty encourages sparsity, meaning that Lasso can eliminate irrelevant features entirely by setting their corresponding weights to zero. This makes Lasso an effective method for feature selection in high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:#2E86C1\"><b>Elastic Net Regression:(Combination of Ridge and Lasso)</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net Regression** combines both Lasso and Ridge regression to achieve a balance between feature selection and regularization. It is particularly useful when dealing with highly correlated features.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Elastic Net Loss Function</b></span>\n",
    "\n",
    "The **Elastic Net Loss** function integrates the **Mean Squared Error (MSE)** with both L1 and L2 penalty terms:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Elastic Net}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\left( l_1 \\sum_{j=1}^{p} |w_j| + (1 - l_1) \\sum_{j=1}^{p} w_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\lambda $\\) is the **regularization parameter** (similar to Lasso).\n",
    "- \\($ l_1 $\\) is the **l1_ratio**, which controls the balance between Lasso and Ridge penalties (0 ≤ l1_ratio ≤ 1).\n",
    "- \\($ |w_j| $\\) is the **absolute sum of the weights** (L1 penalty).\n",
    "- \\($ w_j^2 $\\) is the **sum of squares of the weights** (L2 penalty).\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Elastic Net Loss with Respect to Weight \\($ w $\\)</b></span>\n",
    "\n",
    "To derive the weight update rule for Elastic Net Regression, we compute the **gradient of the Elastic Net Loss** with respect to the weight \\($ w $\\). The gradient consists of three components:\n",
    "\n",
    "- **Gradient of the MSE**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the L1 regularization term** (Lasso):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\lambda l_1 \\sum_{j=1}^{p} |w|)}{\\partial w} = \\lambda l_1 \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "- **Gradient of the L2 regularization term** (Ridge):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\lambda (1 - l_1) \\sum_{j=1}^{p} w^2\\right)}{\\partial w} = 2\\lambda (1 - l_1) w\n",
    "$$\n",
    "\n",
    "The total gradient for **Elastic Net Regression** is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Elastic Net}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda l_1 \\cdot \\text{sgn}(w) + 2\\lambda (1 - l_1) w\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Elastic Net Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\($ w $\\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) + 2\\lambda (1 - l_1) w_{\\text{old}} \\right)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - \\alpha \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) - 2\\alpha \\lambda (1 - l_1) w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. Key Differences from Lasso and Ridge Regression</b></span>\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Combination of Penalties</b></span>\n",
    "\n",
    "- Elastic Net combines L1 and L2 penalties, allowing it to benefit from both feature selection (L1) and regularization (L2).\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>l1_ratio Parameter</b></span>\n",
    "\n",
    "- The **l1_ratio** parameter controls the balance between Lasso and Ridge regularization:\n",
    "  - If \\($ l_1 = 1 $\\), Elastic Net behaves like Lasso.\n",
    "  - If \\($ l_1 = 0 $\\), Elastic Net behaves like Ridge.\n",
    "  - Values between 0 and 1 provide a mix of both.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Weight Updates in Elastic Net Regression</b></span>\n",
    "\n",
    "In Elastic Net Regression, the weight update rule incorporates both L1 and L2 regularization terms, balanced by the **l1_ratio**. Let's break down the impacts:\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding the Components</b></span>\n",
    "\n",
    "1. **L1 Regularization Term**:\n",
    "   - The term \\($ \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) $\\) reduces the weights.\n",
    "  \n",
    "2. **L2 Regularization Term**:\n",
    "   - The term \\($ 2\\lambda (1 - l_1) w_{\\text{old}} $\\) penalizes larger weights, encouraging weight decay.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact on Weight Updates</b></span>\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Positive**:\n",
    "  - The L1 term reduces the weight, while the L2 term further encourages smaller weights.\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Negative**:\n",
    "  - The L1 term increases the weight, pushing it closer to zero, while the L2 term counteracts by promoting decay.\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Small (Close to Zero)**:\n",
    "  - Both regularization terms work together to drive the weight towards zero, allowing for effective feature selection.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Overall Effect on Feature Selection</b></span>\n",
    "\n",
    "The Elastic Net regression encourages sparsity and feature selection while retaining some ability to handle correlated features due to the inclusion of the L2 penalty. This makes it an effective choice in scenarios where there are many features, some of which may be highly correlated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nothing yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
