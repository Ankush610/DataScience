{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Linear Regression Model: A Deep Dive</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear regression is a fundamental algorithm used in both **machine learning (ML)** and **deep learning (DL)**. It models the relationship between a dependent variable \\($ y $\\) and an independent variable \\($ x $\\) by fitting a **linear equation**. Let's break down the model step by step.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Basic Model Architecture</b></span>\n",
    "\n",
    "In a **deep learning** context, linear regression can be viewed as a **simple neural network** with:\n",
    "\n",
    "- **One input** (the feature \\($ x $\\)),\n",
    "- **One neuron** (which applies a linear transformation \\($ y = wx + b $\\)),\n",
    "- **One output** (the predicted value \\($ \\hat{y} $\\)).\n",
    "\n",
    "The equation for linear regression is:\n",
    "\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ w $\\) is the **weight** (or slope of the line),\n",
    "- \\($ b $\\) is the **bias** (the y-intercept of the line),\n",
    "- \\($ x $\\) is the **input feature**,\n",
    "- \\($ \\hat{y} $\\) is the **predicted output**.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Loss Function</b></span>\n",
    "\n",
    "The goal of linear regression is to minimize the error between the predicted values \\($ \\hat{y} $\\) and the actual values \\($ y $\\). This error is quantified using a **loss function**. The most common loss function for linear regression is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ n $\\) is the **number of data points**,\n",
    "- \\($ y_i $\\) is the **actual value** of the \\($ i $\\)-th data point,\n",
    "- \\($ \\hat{y}_i $\\) is the **predicted value**.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Gradient Descent (Weight and Bias Updates)</b></span>\n",
    "\n",
    "To minimize the loss function, we use **gradient descent**. This optimization algorithm updates the weights and biases by computing the **gradients** of the loss function with respect to these parameters.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Understanding Gradient Descent</b></span>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. What is a Gradient?</b></span>\n",
    "\n",
    "- The **gradient** is a vector that points in the direction of the **steepest ascent** of a function.\n",
    "- It indicates how much the function will increase (or decrease) if you move in that direction.\n",
    "- In simple terms, it gives you the **slope** of the function at a specific point.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. What is Gradient Descent?</b></span>\n",
    "\n",
    "- **Gradient Descent** is an optimization algorithm used to minimize a function (often a loss function in machine learning).\n",
    "- It involves taking **steps downhill** towards the minimum point of the function.\n",
    "- The algorithm uses the gradient to determine the direction to move and how far to go.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. What is a Partial Derivative?</b></span>\n",
    "\n",
    "- A **partial derivative** measures how a function changes as one of its variables changes while keeping other variables constant.\n",
    "- It allows us to understand the **sensitivity** of the function to each parameter independently.\n",
    "- In the context of gradient descent, partial derivatives are used to calculate the gradient.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>4. Key Concepts</b></span>\n",
    "\n",
    "- **Steepest Ascent vs. Steepest Descent**:\n",
    "  - The **gradient** indicates the direction of steepest ascent.\n",
    "  - **Gradient Descent** uses the negative gradient to find the direction of steepest descent.\n",
    "  \n",
    "- **Learning Rate**:\n",
    "  - The **learning rate** \\( $\\alpha$ \\) controls the size of the steps taken during the descent.\n",
    "  - A small learning rate leads to slow convergence, while a large learning rate may overshoot the minimum.\n",
    "\n",
    "- **Iteration**:\n",
    "  - Gradient descent is performed iteratively, updating the weights or parameters until convergence (when the updates become negligibly small).\n",
    "\n",
    "<center><img src=\"../../../images/gradient_descent.png\" alt=\"error\" width=\"800\"/></center>\n",
    "---\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Gradient of the Loss Function</b></span>\n",
    "\n",
    "We compute the **partial derivatives** of the loss function with respect to both \\($ w $\\) (weight) and \\($ b $\\) (bias):\n",
    "\n",
    "- **Partial derivative w.r.t \\($ w $\\)**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Partial derivative w.r.t \\($ b $\\)**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Updating Weight and Bias</b></span>\n",
    "\n",
    "The **gradient descent** algorithm updates \\($ w $\\) and \\($ b $\\) iteratively using the **learning rate** \\($ \\alpha $\\) (which controls the step size of the update):\n",
    "\n",
    "- **Weight update**:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "- **Bias update**:\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "These updates are performed over multiple **epochs** (iterations over the entire dataset) until the loss converges to a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Key Terms Explained</b></span>\n",
    "\n",
    "- **Loss**: This is a measure of how far off the predicted values \\($ \\hat{y} $\\) are from the actual values \\($ y $\\). In linear regression, the loss is often calculated using the **MSE**.\n",
    "\n",
    "- **Gradient Descent**: This is the optimization algorithm used to minimize the loss function by adjusting the model's parameters (**weight** and **bias**). It works by moving in the direction of the steepest decrease of the loss (negative gradient).\n",
    "\n",
    "- **Epoch**: An epoch refers to one complete pass through the entire training dataset. In each epoch, the model updates its weights and biases using gradient descent.\n",
    "\n",
    "- **Weight (w)**: This is the parameter that determines how much influence the input variable \\($ x $\\) has on the output \\($ y $\\). It's equivalent to the slope of the line in linear regression.\n",
    "\n",
    "- **Bias (b)**: This is the parameter that shifts the line up or down. It's equivalent to the y-intercept in linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Calculating the Updated Weight (w)</b></span>\n",
    "\n",
    "Let's now dive deeper into the **equation** to calculate the updated **weight** \\($ w $\\). We will substitute values step by step to get a final formula.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Starting Point</b></span>\n",
    "\n",
    "We are using **gradient descent** to update \\($ w $\\):\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\alpha $\\) is the **learning rate**,\n",
    "- \\($ \\frac{\\partial \\text{Loss}}{\\partial w} $\\) is the **gradient of the loss** with respect to \\($ w $\\).\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Loss Function and Gradient</b></span>\n",
    "\n",
    "For linear regression, the predicted value \\($ \\hat{y} $\\) is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "And the loss is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Substitute \\($ \\hat{y}_i = w \\cdot x_i + b $\\) into the loss function:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b))^2\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Gradient Calculation</b></span>\n",
    "\n",
    "Take the partial derivative of the loss function with respect to \\($ w $\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - (w \\cdot x_i + b))\n",
    "$$\n",
    "\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Updating weight (w)</b></span>\n",
    "\n",
    "Finally, the weight update is:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left( -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b)) \\right)\n",
    "$$\n",
    "\n",
    "This formula updates the weight \\($ w $\\) by moving in the direction of the **steepest descent** (negative gradient) to minimize the loss.\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Calculating the Updated Bias (b)</b></span>\n",
    "\n",
    "Just like with the weight \\($ w $\\), we use **gradient descent** to update the **bias** \\($ b $\\). The formula is:\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\alpha $\\) is the **learning rate**,\n",
    "- \\($ \\frac{\\partial \\text{Loss}}{\\partial b} $\\) is the **gradient of the loss** with respect to \\($ b $\\).\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Loss Function and Bias Gradient</b></span>\n",
    "\n",
    "For linear regression, the predicted value \\($ \\hat{y} $\\) is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "Substituting \\($ \\hat{y}_i = w \\cdot x_i + b $\\) into the **Mean Squared Error (MSE)** loss function:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b))^2\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Gradient Calculation for Bias</b></span>\n",
    "\n",
    "Now, we take the **partial derivative** of the loss function with respect to \\($ b $\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b))\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Updating Bias (b)</b></span>\n",
    "\n",
    "Finally, the bias update is:\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\left( -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b)) \\right)\n",
    "$$\n",
    "\n",
    "This formula updates the bias \\($ b $\\) by moving in the direction of the **steepest descent** (negative gradient) to minimize the loss.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
