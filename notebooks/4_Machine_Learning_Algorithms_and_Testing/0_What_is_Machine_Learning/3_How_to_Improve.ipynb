{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create a model we need to know how to improve accuracy of models we made. It's a complicated step and it does not always gives the result we wanted. The key idea is trying out different things we have to see if they have positive impact or negative impact on models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Scaling Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " \n",
    "- ## <span style=\"color:#D35400\"><b>Normalization</b></span>\n",
    "    \n",
    "    Normalization refers to the process of scaling individual samples to have unit norm. This means that the feature values are scaled to fit within a specific range, often between 0 and 1. Normalization is useful when the feature values have different ranges and you want to bring them to a common scale.\n",
    "    \n",
    "    **Example**: Let's say the weight and price of gold; one scale is very small while the other is very large.\n",
    "\n",
    "    **Formula and Notation**:\n",
    "    \n",
    "    $$ \n",
    "    X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    - $X'$ = Normalized value\n",
    "    - $X$ = Original value\n",
    "    - $X_{min}$ = Minimum value of the feature\n",
    "    - $X_{max}$ = Maximum value of the feature\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    ```\n",
    "\n",
    "    **When to use**:\n",
    "    \n",
    "    Normalization is generally preferred when the features have different scales, particularly when using algorithms that rely on distances, such as **k-nearest neighbors (KNN)** and **neural networks**.\n",
    "\n",
    "--- \n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Standardization</b></span>\n",
    "    \n",
    "    Standardization (or Z-score normalization) transforms the data to have a mean of 0 and a standard deviation of 1. This process ensures that the feature distribution follows a standard normal distribution, which is useful for algorithms that assume normally distributed data.\n",
    "\n",
    "    **Formula and Notation**:\n",
    "    \n",
    "    $$ \n",
    "    X' = \\frac{X - \\mu}{\\sigma} \n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    - $X'$ = Standardized value\n",
    "    - $X$ = Original value\n",
    "    - $\\mu$ = Mean of the feature\n",
    "    - $\\sigma$ = Standard deviation of the feature\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(data)\n",
    "    ```\n",
    "\n",
    "    **When to use**:\n",
    "    \n",
    "    Standardization is more appropriate when the data follows a **Gaussian distribution**, especially when using algorithms like **linear regression**, **logistic regression**, and **support vector machines (SVM)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Imputing Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Data imputation is a method for retaining the majority of the dataset's information by substituting missing data with different values. These methods are employed because it would be impractical to remove data from a dataset each time a missing value is encountered. Imputation helps in maintaining the integrity of the dataset and avoiding potential biases introduced by removing data.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Different Techniques</b></span>\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Imputing with Mean, Median, Mode, Forward Fill (ffill), and Backward Fill (bfill)</b></span>\n",
    "    \n",
    "        You can use the `fillna()` method from pandas to impute missing values in various ways.\n",
    "        \n",
    "        ```bash\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Sample DataFrame\n",
    "        data = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 3, 4, None]})\n",
    "        \n",
    "        # Impute with mean\n",
    "        data['A'].fillna(data['A'].mean(), inplace=True)\n",
    "\n",
    "        # Impute with median\n",
    "        data['B'].fillna(data['B'].median(), inplace=True)\n",
    "\n",
    "        # Impute with mode\n",
    "        data['B'].fillna(data['B'].mode()[0], inplace=True)\n",
    "\n",
    "        # Forward fill\n",
    "        data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Backward fill\n",
    "        data.fillna(method='bfill', inplace=True)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Iterative Imputation (MICE)</b></span>\n",
    "        \n",
    "        - ### <span style=\"color:pink\"><b>Overview</b></span>\n",
    "            -  Multiple Imputation by Chained Equations (MICE) uses an iterative approach to fill in missing values based on other features.\n",
    "            - Utilizes a regression model to predict missing values based on other features in the dataset.\n",
    "        \n",
    "        - ### <span style=\"color:pink\"><b>Process</b></span>\n",
    "            - Initializes missing values with a guess (e.g., mean).\n",
    "            - Iteratively models each feature with missing values using regression on remaining features.\n",
    "            - Updates missing values until convergence.\n",
    "\n",
    "        - ### <span style=\"color:pink\"><b>Benefits</b></span>\n",
    "            - Captures complex relationships among features for more accurate imputations.\n",
    "            - Suitable for datasets with correlated features.\n",
    "\n",
    "        ```bash\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "\n",
    "        imputer = IterativeImputer()\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>KNN Imputation</b></span>\n",
    "    \n",
    "        - ### <span style=\"color:pink\"><b>Overview</b></span>\n",
    "            - K-Nearest Neighbors (KNN) can also be used to impute missing values based on the nearest samples.\n",
    "            - Fills missing values by averaging values from the K nearest neighbors in the dataset.\n",
    "    \n",
    "        - ### <span style=\"color:pink\"><b>Process</b></span>\n",
    "            - Calculates distance between instances to find K nearest neighbors.\n",
    "            - Imputes missing values using the mean (for continuous features) or mode (for categorical features) of the neighbors.\n",
    "\n",
    "        - ### <span style=\"color:pink\"><b>Benefits</b></span>\n",
    "            - Preserves local structure and relationships in the data.\n",
    "            - Simple and effective when sufficient similar observations are present.\n",
    "            \n",
    "\n",
    "        ```bash\n",
    "        from sklearn.impute import KNNImputer\n",
    "        \n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Simple Imputer</b></span>\n",
    "    \n",
    "        The `SimpleImputer` class can be used to specify different strategies for imputation.\n",
    "        \n",
    "        ```bash\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent', etc.\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Imputing with Min/Max Values</b></span>\n",
    "    \n",
    "        You can also use the minimum or maximum values for imputation.\n",
    "        \n",
    "        ```bash\n",
    "        # Impute with minimum value\n",
    "        data.fillna(data.min(), inplace=True)\n",
    "\n",
    "        # Impute with maximum value\n",
    "        data.fillna(data.max(), inplace=True)\n",
    "        ```\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>When to Use Each Technique</b></span>\n",
    "\n",
    "    - **Mean, Median, Mode Imputation**: Use these methods for numerical features when the data is symmetrically distributed. Median is preferable for skewed distributions.\n",
    "    - **Forward Fill / Backward Fill**: Suitable for time series data where the order is important, and missing values are expected to be similar to nearby values.\n",
    "    - **Iterative Imputation (MICE)**: Best for datasets with complex relationships among features. This technique often yields better results when features are correlated.\n",
    "    - **KNN Imputation**: Effective for datasets where the values of a feature are influenced by other features. It's useful when the dataset is not too large, as it can be computationally expensive.\n",
    "    - **Simple Imputer**: Useful for general cases where a specific strategy is required. It offers flexibility in choosing the imputation strategy.\n",
    "    - **Min/Max Imputation**: Generally used for bounded features, but use with caution as it can reduce variability in the data.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Why We Need Imputation</b></span>\n",
    "\n",
    "    Imputation is crucial for maintaining the usability of a dataset, especially in real-world applications where missing values are common. By imputing missing data, we can preserve the size and integrity of the dataset, which is vital for training effective machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Impact on Actual Data and Model</b></span>\n",
    "\n",
    "    The impact of data imputation can be both positive and negative:\n",
    "    \n",
    "    - **Positive**: Imputation can lead to more robust models that generalize better due to the increased amount of usable data.\n",
    "    - **Negative**: If not done correctly, imputation can introduce bias, reduce variability, or distort relationships between features, ultimately leading to poor model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Regularization</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Regularization is a set of methods aimed at **reducing overfitting** in machine learning models. Typically, it involves trading a marginal decrease in **training accuracy** for an increase in **generalizability**—the model's ability to produce accurate predictions on new datasets.\n",
    "-   Basically, regularization increases a model’s generalizability but often results in **higher training error**. This means models may perform less accurately on training data while improving predictions on test data.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Bias-Variance Tradeoff</b></span>\n",
    "\n",
    "The concession of increased training error for decreased testing error is known as the **bias-variance tradeoff**. Here's a brief breakdown:\n",
    "\n",
    "- **Bias**: Measures the average difference between predicted and true values. High bias results in high error on the training set.\n",
    "  \n",
    "- **Variance**: Measures how much predictions differ across various subsets of the same data. High variance indicates poor performance on unseen data.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Key Points on Variance:</b></span>\n",
    "- **Variance** in machine learning reflects how much a model's predictions change when trained on different data subsets. It signifies a model's sensitivity to training data.\n",
    "\n",
    "- **Different Subsets, Different Models**: Training on different data subsets often results in slightly different models due to randomness.\n",
    "  \n",
    "- **Prediction Variation**: These models may produce varying predictions on unseen data, with variance measuring the extent of this variation.\n",
    "\n",
    "- **Lower Prediction Variance**: Indicates that the model generalizes well rather than memorizing patterns.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Aim of Regularization:</b></span>\n",
    "\n",
    "Developers strive to reduce both bias and variance. However, simultaneous reduction isn't always achievable, leading to the need for regularization, which decreases model variance at the cost of increased bias.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding Overfitting and Underfitting:</b></span>\n",
    "\n",
    "- **Overfitting**: Characterized by low bias and high variance. This occurs when a model learns noise from the training data.\n",
    "  \n",
    "- **Underfitting**: Refers to high bias and high variance, resulting in poor predictions on both training and test data. This often arises from insufficient training data or parameters.\n",
    "\n",
    "<center><img src=\"../../../images/bias_variance_tradeoff.jpg\" alt=\"error\" width=\"600\"/></center>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Regularization Effects:</b></span>\n",
    "\n",
    "While regularization aims to reduce overfitting, it can also lead to underfitting if too much bias is introduced. Thus, determining the appropriate type and degree of regularization requires careful consideration of:\n",
    "\n",
    "- Model complexity\n",
    "- Dataset characteristics\n",
    "- Specific requirements of the task\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
