{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create a model we need to know how to improve accuracy of models we made. It's a complicated step and it does not always gives the result we wanted. The key idea is trying out different things we have to see if they have positive impact or negative impact on models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Scaling Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " \n",
    "- ## <span style=\"color:#D35400\"><b>Normalization</b></span>\n",
    "    \n",
    "    Normalization refers to the process of scaling individual samples to have unit norm. This means that the feature values are scaled to fit within a specific range, often between 0 and 1. Normalization is useful when the feature values have different ranges and you want to bring them to a common scale.\n",
    "    \n",
    "    **Example**: Let's say the weight and price of gold; one scale is very small while the other is very large.\n",
    "\n",
    "    **Formula and Notation**:\n",
    "    \n",
    "    $$ \n",
    "    X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    - $X'$ = Normalized value\n",
    "    - $X$ = Original value\n",
    "    - $X_{min}$ = Minimum value of the feature\n",
    "    - $X_{max}$ = Maximum value of the feature\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    ```\n",
    "\n",
    "    **When to use**:\n",
    "    \n",
    "    Normalization is generally preferred when the features have different scales, particularly when using algorithms that rely on distances, such as **k-nearest neighbors (KNN)** and **neural networks**.\n",
    "\n",
    "--- \n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Standardization</b></span>\n",
    "    \n",
    "    Standardization (or Z-score normalization) transforms the data to have a mean of 0 and a standard deviation of 1. This process ensures that the feature distribution follows a standard normal distribution, which is useful for algorithms that assume normally distributed data.\n",
    "\n",
    "    **Formula and Notation**:\n",
    "    \n",
    "    $$ \n",
    "    X' = \\frac{X - \\mu}{\\sigma} \n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    - $X'$ = Standardized value\n",
    "    - $X$ = Original value\n",
    "    - $\\mu$ = Mean of the feature\n",
    "    - $\\sigma$ = Standard deviation of the feature\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(data)\n",
    "    ```\n",
    "\n",
    "    **When to use**:\n",
    "    \n",
    "    Standardization is more appropriate when the data follows a **Gaussian distribution**, especially when using algorithms like **linear regression**, **logistic regression**, and **support vector machines (SVM)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Imputing Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Data imputation is a method for retaining the majority of the dataset's information by substituting missing data with different values. These methods are employed because it would be impractical to remove data from a dataset each time a missing value is encountered. Imputation helps in maintaining the integrity of the dataset and avoiding potential biases introduced by removing data.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Different Techniques</b></span>\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Imputing with Mean, Median, Mode, Forward Fill (ffill), and Backward Fill (bfill)</b></span>\n",
    "    \n",
    "        You can use the `fillna()` method from pandas to impute missing values in various ways.\n",
    "        \n",
    "        ```bash\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Sample DataFrame\n",
    "        data = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 3, 4, None]})\n",
    "        \n",
    "        # Impute with mean\n",
    "        data['A'].fillna(data['A'].mean(), inplace=True)\n",
    "\n",
    "        # Impute with median\n",
    "        data['B'].fillna(data['B'].median(), inplace=True)\n",
    "\n",
    "        # Impute with mode\n",
    "        data['B'].fillna(data['B'].mode()[0], inplace=True)\n",
    "\n",
    "        # Forward fill\n",
    "        data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Backward fill\n",
    "        data.fillna(method='bfill', inplace=True)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Iterative Imputation (MICE)</b></span>\n",
    "        \n",
    "        - ### <span style=\"color:pink\"><b>Overview</b></span>\n",
    "            -  Multiple Imputation by Chained Equations (MICE) uses an iterative approach to fill in missing values based on other features.\n",
    "            - Utilizes a regression model to predict missing values based on other features in the dataset.\n",
    "        \n",
    "        - ### <span style=\"color:pink\"><b>Process</b></span>\n",
    "            - Initializes missing values with a guess (e.g., mean).\n",
    "            - Iteratively models each feature with missing values using regression on remaining features.\n",
    "            - Updates missing values until convergence.\n",
    "\n",
    "        - ### <span style=\"color:pink\"><b>Benefits</b></span>\n",
    "            - Captures complex relationships among features for more accurate imputations.\n",
    "            - Suitable for datasets with correlated features.\n",
    "\n",
    "        ```bash\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "\n",
    "        imputer = IterativeImputer()\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>KNN Imputation</b></span>\n",
    "    \n",
    "        - ### <span style=\"color:pink\"><b>Overview</b></span>\n",
    "            - K-Nearest Neighbors (KNN) can also be used to impute missing values based on the nearest samples.\n",
    "            - Fills missing values by averaging values from the K nearest neighbors in the dataset.\n",
    "    \n",
    "        - ### <span style=\"color:pink\"><b>Process</b></span>\n",
    "            - Calculates distance between instances to find K nearest neighbors.\n",
    "            - Imputes missing values using the mean (for continuous features) or mode (for categorical features) of the neighbors.\n",
    "\n",
    "        - ### <span style=\"color:pink\"><b>Benefits</b></span>\n",
    "            - Preserves local structure and relationships in the data.\n",
    "            - Simple and effective when sufficient similar observations are present.\n",
    "            \n",
    "\n",
    "        ```bash\n",
    "        from sklearn.impute import KNNImputer\n",
    "        \n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Simple Imputer</b></span>\n",
    "    \n",
    "        The `SimpleImputer` class can be used to specify different strategies for imputation.\n",
    "        \n",
    "        ```bash\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent', etc.\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Imputing with Min/Max Values</b></span>\n",
    "    \n",
    "        You can also use the minimum or maximum values for imputation.\n",
    "        \n",
    "        ```bash\n",
    "        # Impute with minimum value\n",
    "        data.fillna(data.min(), inplace=True)\n",
    "\n",
    "        # Impute with maximum value\n",
    "        data.fillna(data.max(), inplace=True)\n",
    "        ```\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>When to Use Each Technique</b></span>\n",
    "\n",
    "    - **Mean, Median, Mode Imputation**: Use these methods for numerical features when the data is symmetrically distributed. Median is preferable for skewed distributions.\n",
    "    - **Forward Fill / Backward Fill**: Suitable for time series data where the order is important, and missing values are expected to be similar to nearby values.\n",
    "    - **Iterative Imputation (MICE)**: Best for datasets with complex relationships among features. This technique often yields better results when features are correlated.\n",
    "    - **KNN Imputation**: Effective for datasets where the values of a feature are influenced by other features. It's useful when the dataset is not too large, as it can be computationally expensive.\n",
    "    - **Simple Imputer**: Useful for general cases where a specific strategy is required. It offers flexibility in choosing the imputation strategy.\n",
    "    - **Min/Max Imputation**: Generally used for bounded features, but use with caution as it can reduce variability in the data.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Why We Need Imputation</b></span>\n",
    "\n",
    "    Imputation is crucial for maintaining the usability of a dataset, especially in real-world applications where missing values are common. By imputing missing data, we can preserve the size and integrity of the dataset, which is vital for training effective machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Impact on Actual Data and Model</b></span>\n",
    "\n",
    "    The impact of data imputation can be both positive and negative:\n",
    "    \n",
    "    - **Positive**: Imputation can lead to more robust models that generalize better due to the increased amount of usable data.\n",
    "    - **Negative**: If not done correctly, imputation can introduce bias, reduce variability, or distort relationships between features, ultimately leading to poor model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Regularization</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Regularization is a set of methods aimed at **reducing overfitting** in machine learning models. Typically, it involves trading a marginal decrease in **training accuracy** for an increase in **generalizability**—the model's ability to produce accurate predictions on new datasets.\n",
    "-   Basically, regularization increases a model’s generalizability but often results in **higher training error**. This means models may perform less accurately on training data while improving predictions on test data.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Bias-Variance Tradeoff</b></span>\n",
    "\n",
    "The concession of increased training error for decreased testing error is known as the **bias-variance tradeoff**. Here's a brief breakdown:\n",
    "\n",
    "- **Bias**: Measures the average difference between predicted and true values. High bias results in high error on the training set.\n",
    "  \n",
    "- **Variance**: Measures how much predictions differ across various subsets of the same data. High variance indicates poor performance on unseen data.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Key Points on Variance:</b></span>\n",
    "- **Variance** in machine learning reflects how much a model's predictions change when trained on different data subsets. It signifies a model's sensitivity to training data.\n",
    "\n",
    "- **Different Subsets, Different Models**: Training on different data subsets often results in slightly different models due to randomness.\n",
    "  \n",
    "- **Prediction Variation**: These models may produce varying predictions on unseen data, with variance measuring the extent of this variation.\n",
    "\n",
    "- **Lower Prediction Variance**: Indicates that the model generalizes well rather than memorizing patterns.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Aim of Regularization:</b></span>\n",
    "\n",
    "Developers strive to reduce both bias and variance. However, simultaneous reduction isn't always achievable, leading to the need for regularization, which decreases model variance at the cost of increased bias.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding Overfitting and Underfitting:</b></span>\n",
    "\n",
    "- **Overfitting**: \n",
    "    -   Characterized by low bias and high variance. This occurs when a model learns noise from the training data.\n",
    "    -   Happens when the model is too complex and captures even the noise in the data, making it perform well on the training data but poorly on unseen data.\n",
    "- **Underfitting**: \n",
    "    -   Refers to high bias and high variance, resulting in poor predictions on both training and test data. This often arises from insufficient training data or parameters.\n",
    "    -   Occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "---\n",
    "\n",
    "- ### <span style=\"color:#D35400\"><b>Impact of Data Size on Underfitting and Overfitting</b></span>\n",
    "\n",
    "    - #### <span style=\"color:#28B463\"><b>1. Small Data Size</b></span>\n",
    "\n",
    "        When the dataset is small, **overfitting** is more likely to occur because the model memorizes the limited data points and fails to generalize to new data.\n",
    "\n",
    "        `Small data`: Models may learn specific details (including noise) and struggle when exposed to new data.\n",
    "\n",
    "    - #### <span style=\"color:#28B463\"><b>2. Large Data Size</b></span>\n",
    "\n",
    "        With **more data**, the risk of **overfitting decreases**, as the model has a larger, more diverse set of examples to learn from. However, with a simple model, underfitting might occur because the model cannot capture the complexity of the larger dataset.\n",
    "\n",
    "---\n",
    "\n",
    "- ### <span style=\"color:#D35400\"><b>Balancing the Data and Model Complexity</b></span>\n",
    "\n",
    "    - **Larger datasets** generally help reduce overfitting because the model can generalize better. However, to prevent underfitting, **model complexity** should increase with the size of the dataset.\n",
    "\n",
    "    - Proper techniques such as **cross-validation**, **regularization**, and **model tuning** are crucial to ensuring that the model neither underfits nor overfits, regardless of the data size.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src=\"../../../images/bias_variance_tradeoff.jpg\" alt=\"error\" width=\"600\"/></center>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Regularization Effects:</b></span>\n",
    "\n",
    "While regularization aims to reduce overfitting, it can also lead to underfitting if too much bias is introduced. Thus, determining the appropriate type and degree of regularization requires careful consideration of:\n",
    "\n",
    "- Model complexity\n",
    "- Dataset characteristics\n",
    "- Specific requirements of the task\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Ridge Regression: Introducing Regularization</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Ridge Regression**, the primary difference from ordinary linear regression is the inclusion of a **regularization term**. This term helps penalize large weights to prevent **overfitting**, leading to a model that generalizes better.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Ridge Regression Loss Function</b></span>\n",
    "\n",
    "The **Ridge Regression Loss** function combines the **Mean Squared Error (MSE)** with a penalty term that controls the magnitude of the weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Ridge}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda w^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( $\\lambda$ \\) is the **regularization parameter** (often referred to as **alpha** in Ridge, which should not be confused with the learning rate \\( $\\alpha$ \\)),\n",
    "- \\( $w^2$ \\) is the **sum of the squared weights**.\n",
    "\n",
    "The additional term \\( $\\lambda w^2$ \\) discourages the model from learning large weights, which may lead to overfitting.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Ridge Loss with Respect to Weight \\( $w$ \\)</b></span>\n",
    "\n",
    "To derive the weight update rule, we need to compute the **gradient of the Ridge Loss** with respect to the weight \\( $w$ \\). This consists of two parts:\n",
    "\n",
    "- **Gradient of the MSE** (same as in linear regression):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the regularization term \\( \\lambda w^2 \\)**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda w^2}{\\partial w} = 2\\lambda w\n",
    "$$\n",
    "\n",
    "Thus, the total gradient for **Ridge Regression** becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Ridge}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + 2\\lambda w\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Ridge Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\( w \\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + 2\\lambda w_{\\text{old}} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying the update formula:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - 2\\alpha\\lambda w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "This final equation shows how the weight is updated at each step of gradient descent in **Ridge Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Key Differences from Ordinary Linear Regression</b></span>\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Regularization Term</b></span>\n",
    "\n",
    "The key difference in Ridge Regression is the inclusion of the **second term** \\( $2\\alpha\\lambda w_{\\text{old}}$ \\) in the weight update rule. This term penalizes large values of \\( $w$ \\), shrinking the weights over time and helping to prevent **overfitting**.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Regularization Parameter \\( $\\lambda$ \\)</b></span>\n",
    "\n",
    "- The **regularization parameter \\( $\\lambda$ \\)** controls the strength of the penalty. \n",
    "- When \\( $\\lambda$ = 0 \\), Ridge Regression becomes equivalent to **ordinary linear regression**. \n",
    "- A larger \\( $\\lambda$ \\) results in greater penalization, pushing the weights towards zero and reducing model complexity.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact on Generalization</b></span>\n",
    "\n",
    "The regularization term \\( $\\lambda w^2$ \\) encourages the model to have **smaller weights**, preventing it from overfitting the training data. This allows the model to generalize better to unseen data, avoiding **overly complex solutions** that fit noise in the data.\n",
    "\n",
    "---\n",
    "\n",
    "By adding **Ridge regularization**, we improve the **stability** of the linear model, especially when dealing with **multicollinearity** (where predictor variables are highly correlated). Ridge Regression is an effective tool when you need to balance between fitting your data and maintaining a model that generalizes well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Lasso Regression: Emphasizing Feature Selection</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Lasso Regression**, the key difference from ordinary linear regression is the introduction of a **regularization term** that encourages sparsity in the model. This means that some coefficients can become exactly zero, leading to a simpler and more interpretable model.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Lasso Regression Loss Function</b></span>\n",
    "\n",
    "The **Lasso Regression Loss** function integrates the **Mean Squared Error (MSE)** with a penalty term based on the absolute values of the weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Lasso}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |w_j|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\lambda $\\) is the **regularization parameter** (also called **alpha** in Lasso, which should not be confused with the learning rate \\($ \\alpha $\\)),\n",
    "- \\($ |w_j| $\\) is the **absolute sum of the weights**.\n",
    "\n",
    "The term \\($ \\lambda \\sum_{j=1}^{p} |w_j| $\\) encourages some weights to shrink to zero, effectively performing feature selection.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Lasso Loss with Respect to Weight \\($ w $\\)</b></span>\n",
    "\n",
    "To derive the weight update rule for Lasso Regression, we compute the **gradient of the Lasso Loss** with respect to the weight \\($ w $\\). The gradient consists of two components:\n",
    "\n",
    "- **Gradient of the MSE** (same as in linear regression):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the regularization term \\($ \\lambda |w| $\\)**:\n",
    "\n",
    "The derivative with respect to \\($ w $\\) involves the **sign function**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda |w|}{\\partial w} = \\lambda \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "So, the total gradient for **Lasso Regression** is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Lasso}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Lasso Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\($ w $\\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) \\right)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - \\alpha \\lambda \\cdot \\text{sgn}(w_{\\text{old}})\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. Key Differences from Ordinary Linear Regression</b></span>\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Regularization Term</b></span>\n",
    "\n",
    "The major distinction in Lasso Regression is the inclusion of the **absolute value term** \\( \\alpha \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) \\) in the weight update rule. This term can drive some weights exactly to zero, allowing the model to exclude less important features.\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Regularization Parameter \\( \\lambda \\)</b></span>\n",
    "\n",
    "- The **regularization parameter \\($ \\lambda $\\)** controls the strength of the penalty.\n",
    "- When \\($ \\lambda = 0 $\\), Lasso Regression becomes equivalent to **ordinary linear regression**.\n",
    "- A larger \\($ \\lambda $\\) increases the penalty, promoting more weights to become zero and leading to a simpler model.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Weight Updates in Lasso Regression</b></span>\n",
    "\n",
    "In Lasso Regression, the weight update rule incorporates the term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\). This term plays a critical role in how the weights are adjusted during training. Let's break down its impact:\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding the Penalty Term</b></span>\n",
    "\n",
    "- **\\($ \\lambda $\\)**: This is the regularization parameter that controls the strength of the penalty. A larger \\($ \\lambda $\\) encourages more weights to shrink towards zero.\n",
    "  \n",
    "- **\\($ \\text{sgn}(w_{\\text{old}})$\\)**: The sign function returns:\n",
    "  - **1** if \\($ w_{\\text{old}} > 0 $\\) (positive weight)\n",
    "  - **-1** if \\($ w_{\\text{old}} < 0 $\\) (negative weight)\n",
    "  - **0** if \\($ w_{\\text{old}} = 0 $\\) (zero weight)\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact of the Penalty Term</b></span>\n",
    "\n",
    "1. **When \\($ w_{\\text{old}} $\\) is Positive**:\n",
    "   - The term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\) contributes positively to the weight update.\n",
    "   - **Effect**: The penalty reduces the value of the weight \\($ w_{\\text{new}} $\\). \n",
    "   - **Interpretation**: This encourages the weight to shrink, thus regularizing the model.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left(\\text{penalty}\\right) \\quad \\text{(penalty is positive)}\n",
    "   $$\n",
    "\n",
    "2. **When \\($ w_{\\text{old}} $\\) is Negative**:\n",
    "   - The term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\) contributes negatively to the weight update.\n",
    "   - **Effect**: The penalty increases the value of the weight \\($ w_{\\text{new}} $\\) (making it less negative).\n",
    "   - **Interpretation**: This adjustment reduces the magnitude of the negative weight, pushing it closer to zero.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left(\\text{penalty}\\right) \\quad \\text{(penalty is negative)}\n",
    "   $$\n",
    "\n",
    "3. **When \\($ w_{\\text{old}} $\\) is Small (Close to Zero)**:\n",
    "   - If \\($ |w_{\\text{old}}| $\\) is small enough, the penalty can effectively drive the weight to exactly zero.\n",
    "   - **Effect**: The weight \\($ w_{\\text{new}} $\\) becomes zero, effectively eliminating that feature from the model.\n",
    "   - **Interpretation**: This feature selection property is a key benefit of Lasso Regression.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = 0 \\quad \\text{(if the update drives \\( w_{\\text{old}} \\) to zero)}\n",
    "   $$\n",
    "\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Impact on Feature Selection</b></span>\n",
    "\n",
    "The L1 penalty encourages sparsity, meaning that Lasso can eliminate irrelevant features entirely by setting their corresponding weights to zero. This makes Lasso an effective method for feature selection in high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:#2E86C1\"><b>Elastic Net Regression:(Combination of Ridge and Lasso)</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net Regression** combines both Lasso and Ridge regression to achieve a balance between feature selection and regularization. It is particularly useful when dealing with highly correlated features.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Elastic Net Loss Function</b></span>\n",
    "\n",
    "The **Elastic Net Loss** function integrates the **Mean Squared Error (MSE)** with both L1 and L2 penalty terms:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Elastic Net}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\left( l_1 \\sum_{j=1}^{p} |w_j| + (1 - l_1) \\sum_{j=1}^{p} w_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\lambda $\\) is the **regularization parameter** (similar to Lasso).\n",
    "- \\($ l_1 $\\) is the **l1_ratio**, which controls the balance between Lasso and Ridge penalties (0 ≤ l1_ratio ≤ 1).\n",
    "- \\($ |w_j| $\\) is the **absolute sum of the weights** (L1 penalty).\n",
    "- \\($ w_j^2 $\\) is the **sum of squares of the weights** (L2 penalty).\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Elastic Net Loss with Respect to Weight \\($ w $\\)</b></span>\n",
    "\n",
    "To derive the weight update rule for Elastic Net Regression, we compute the **gradient of the Elastic Net Loss** with respect to the weight \\($ w $\\). The gradient consists of three components:\n",
    "\n",
    "- **Gradient of the MSE**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the L1 regularization term** (Lasso):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\lambda l_1 \\sum_{j=1}^{p} |w|)}{\\partial w} = \\lambda l_1 \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "- **Gradient of the L2 regularization term** (Ridge):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\lambda (1 - l_1) \\sum_{j=1}^{p} w^2\\right)}{\\partial w} = 2\\lambda (1 - l_1) w\n",
    "$$\n",
    "\n",
    "The total gradient for **Elastic Net Regression** is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Elastic Net}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda l_1 \\cdot \\text{sgn}(w) + 2\\lambda (1 - l_1) w\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Elastic Net Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\($ w $\\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) + 2\\lambda (1 - l_1) w_{\\text{old}} \\right)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - \\alpha \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) - 2\\alpha \\lambda (1 - l_1) w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. Key Differences from Lasso and Ridge Regression</b></span>\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Combination of Penalties</b></span>\n",
    "\n",
    "- Elastic Net combines L1 and L2 penalties, allowing it to benefit from both feature selection (L1) and regularization (L2).\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>l1_ratio Parameter</b></span>\n",
    "\n",
    "- The **l1_ratio** parameter controls the balance between Lasso and Ridge regularization:\n",
    "  - If \\($ l_1 = 1 $\\), Elastic Net behaves like Lasso.\n",
    "  - If \\($ l_1 = 0 $\\), Elastic Net behaves like Ridge.\n",
    "  - Values between 0 and 1 provide a mix of both.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Weight Updates in Elastic Net Regression</b></span>\n",
    "\n",
    "In Elastic Net Regression, the weight update rule incorporates both L1 and L2 regularization terms, balanced by the **l1_ratio**. Let's break down the impacts:\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding the Components</b></span>\n",
    "\n",
    "1. **L1 Regularization Term**:\n",
    "   - The term \\($ \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) $\\) reduces the weights.\n",
    "  \n",
    "2. **L2 Regularization Term**:\n",
    "   - The term \\($ 2\\lambda (1 - l_1) w_{\\text{old}} $\\) penalizes larger weights, encouraging weight decay.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact on Weight Updates</b></span>\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Positive**:\n",
    "  - The L1 term reduces the weight, while the L2 term further encourages smaller weights.\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Negative**:\n",
    "  - The L1 term increases the weight, pushing it closer to zero, while the L2 term counteracts by promoting decay.\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Small (Close to Zero)**:\n",
    "  - Both regularization terms work together to drive the weight towards zero, allowing for effective feature selection.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Overall Effect on Feature Selection</b></span>\n",
    "\n",
    "The Elastic Net regression encourages sparsity and feature selection while retaining some ability to handle correlated features due to the inclusion of the L2 penalty. This makes it an effective choice in scenarios where there are many features, some of which may be highly correlated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Regularization in Deep Learning</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:#D35400\"><b>4. Dropout Regularization (Neural Networks)</b></span>\n",
    "\n",
    "- **Explanation**: **Dropout** is a regularization technique primarily used in neural networks. During training, randomly selected neurons are \"dropped\" or ignored, preventing the model from becoming too dependent on particular neurons and reducing overfitting.\n",
    "  \n",
    "- **How It Works**: Neurons are randomly set to zero during each training step, which forces the model to learn more robust representations.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model.add(Dropout(0.5))\n",
    "```\n",
    "\n",
    "- **Use Case**: Especially useful in **deep learning models** to prevent overfitting, particularly in large networks.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>5. Early Stopping</b></span>\n",
    "\n",
    "- **Explanation**: **Early stopping** halts the training process when the performance on a validation dataset starts to degrade. This prevents the model from continuing to fit the noise in the training data.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "- **Use Case**: Commonly used in **deep learning** to reduce overfitting when training for a large number of epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>6. Data Augmentation (Deep Learning)</b></span>\n",
    "\n",
    "- **Explanation**: **Data Augmentation** increases the size of the training dataset by applying transformations (rotations, flips, etc.) to existing data. It’s a form of regularization that forces the model to learn more robust features by exposing it to slightly varied data.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "```\n",
    "\n",
    "- **Use Case**: Especially effective in **computer vision** tasks when training datasets are small.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>7. Weight Regularization (Neural Networks)</b></span>\n",
    "\n",
    "- **Explanation**: In neural networks, **weight regularization** techniques (like L1 or L2 penalties) are applied to the weights of the network to limit their size, thus preventing overfitting.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.regularizers import l2\n",
    "model.add(Dense(units=64, kernel_regularizer=l2(0.01)))\n",
    "```\n",
    "\n",
    "- **Use Case**: Applied in deep learning networks to control the size of weights and avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "By understanding and applying the right type of regularization, you can control the complexity of your machine learning models, prevent overfitting, and improve generalization on unseen data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
