{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create a model we need to know how to improve accuracy of models we made. It's a complicated step and it does not always gives the result we wanted. The key idea is trying out different things we have to see if they have positive impact or negative impact on models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Scaling Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Scaling data is especially useful for **distance-based models** in machine learning. Models like **K-Means Clustering**, **K-Nearest Neighbors (KNN)**, and **Support Vector Machines (SVM)** rely on distance calculations, and the scale of features can significantly affect their performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  **<span style=\"color:#D35400\">Which Models Require Scaling?</span>**\n",
    "\n",
    "1. **K-Means Clustering**  \n",
    "   - Distance-based, using **Euclidean distance**.\n",
    "   \n",
    "2. **K-Nearest Neighbors (KNN)**  \n",
    "   - Distance-based, where scaling is crucial for meaningful distance calculation.\n",
    "   \n",
    "3. **Support Vector Machines (SVM)**  \n",
    "   - Uses distance-based **Euclidean distance** in the kernel.\n",
    "\n",
    "4. **Principal Component Analysis (PCA)**  \n",
    "   - Uses variance, so feature scaling matters for creating principal components.\n",
    "\n",
    "5. **Neural Networks**  \n",
    "   - Gradient-based learning benefits from scaled inputs for faster convergence.\n",
    "\n",
    "6. **Linear Regression and Logistic Regression**  \n",
    "   - Optional, but helps with faster convergence during optimization.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Normalization</b></span>\n",
    "    \n",
    "    Normalization refers to the process of scaling individual samples to have unit norm. This means that the feature values are scaled to fit within a specific range, often between 0 and 1. Normalization is useful when the feature values have different ranges and you want to bring them to a common scale.\n",
    "    \n",
    "    **Example**: Let's say the weight and price of gold; one scale is very small while the other is very large.\n",
    "\n",
    "    **Formula and Notation**:\n",
    "    \n",
    "    $$ \n",
    "    X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    - $X'$ = Normalized value\n",
    "    - $X$ = Original value\n",
    "    - $X_{min}$ = Minimum value of the feature\n",
    "    - $X_{max}$ = Maximum value of the feature\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    ```\n",
    "\n",
    "    **When to use**:\n",
    "    \n",
    "    Normalization is generally preferred when the features have different scales, particularly when using algorithms that rely on distances, such as **k-nearest neighbors (KNN)** and **neural networks**.\n",
    "\n",
    "--- \n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Standardization</b></span>\n",
    "    \n",
    "    Standardization (or Z-score normalization) transforms the data to have a mean of 0 and a standard deviation of 1. This process ensures that the feature distribution follows a standard normal distribution, which is useful for algorithms that assume normally distributed data.\n",
    "\n",
    "    **Formula and Notation**:\n",
    "    \n",
    "    $$ \n",
    "    X' = \\frac{X - \\mu}{\\sigma} \n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    - $X'$ = Standardized value\n",
    "    - $X$ = Original value\n",
    "    - $\\mu$ = Mean of the feature\n",
    "    - $\\sigma$ = Standard deviation of the feature\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(data)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤” **<span style=\"color:#D35400\">Which Scaling Method is Better?</span>**\n",
    "\n",
    "- **Min-Max Scaling**:  \n",
    "  Best when your data needs to be in a specific range (e.g., [0, 1]).  \n",
    "  Great for **K-Means** and **KNN**, where distance metrics matter.\n",
    "\n",
    "- **Standardization (Z-Score)**:  \n",
    "  Preferred for models like **SVM**, **logistic regression**, and **neural networks**.  \n",
    "  Itâ€™s better for handling outliers and creating normalized distributions.\n",
    "\n",
    "- **Robust Scaling**:  \n",
    "  Best for **datasets with many outliers**. Itâ€™s more resistant to outliers compared to other methods.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **<span style=\"color:#27AE60\">Choosing the Right Scaling Method</span>**\n",
    "\n",
    "- **K-Means, KNN, SVM**:  \n",
    "  Use **Standardization** or **Min-Max Scaling**, depending on the presence of outliers.\n",
    "  \n",
    "- **Neural Networks, Logistic Regression**:  \n",
    "  Standardization is usually preferred, but **Min-Max Scaling** can also be used.\n",
    "\n",
    "- **PCA**:  \n",
    "  **Standardization** is better because it centers the data.\n",
    "\n",
    "---\n",
    "\n",
    "In practice, **Standardization** is the most commonly used method unless the algorithm specifically requires **Min-Max Scaling** (e.g., distance-based models like **KNN** and **K-Means**).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Imputing Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Data imputation is a method for retaining the majority of the dataset's information by substituting missing data with different values. These methods are employed because it would be impractical to remove data from a dataset each time a missing value is encountered. Imputation helps in maintaining the integrity of the dataset and avoiding potential biases introduced by removing data.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Different Techniques</b></span>\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Imputing with Mean, Median, Mode, Forward Fill (ffill), and Backward Fill (bfill)</b></span>\n",
    "    \n",
    "        You can use the `fillna()` method from pandas to impute missing values in various ways.\n",
    "        \n",
    "        ```bash\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Sample DataFrame\n",
    "        data = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 3, 4, None]})\n",
    "        \n",
    "        # Impute with mean\n",
    "        data['A'].fillna(data['A'].mean(), inplace=True)\n",
    "\n",
    "        # Impute with median\n",
    "        data['B'].fillna(data['B'].median(), inplace=True)\n",
    "\n",
    "        # Impute with mode\n",
    "        data['B'].fillna(data['B'].mode()[0], inplace=True)\n",
    "\n",
    "        # Forward fill\n",
    "        data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Backward fill\n",
    "        data.fillna(method='bfill', inplace=True)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Iterative Imputation (MICE)</b></span>\n",
    "        \n",
    "        - ### <span style=\"color:pink\"><b>Overview</b></span>\n",
    "            -  Multiple Imputation by Chained Equations (MICE) uses an iterative approach to fill in missing values based on other features.\n",
    "            - Utilizes a regression model to predict missing values based on other features in the dataset.\n",
    "        \n",
    "        - ### <span style=\"color:pink\"><b>Process</b></span>\n",
    "            - Initializes missing values with a guess (e.g., mean).\n",
    "            - Iteratively models each feature with missing values using regression on remaining features.\n",
    "            - Updates missing values until convergence.\n",
    "\n",
    "        - ### <span style=\"color:pink\"><b>Benefits</b></span>\n",
    "            - Captures complex relationships among features for more accurate imputations.\n",
    "            - Suitable for datasets with correlated features.\n",
    "\n",
    "        ```bash\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "\n",
    "        imputer = IterativeImputer()\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>KNN Imputation</b></span>\n",
    "    \n",
    "        - ### <span style=\"color:pink\"><b>Overview</b></span>\n",
    "            - K-Nearest Neighbors (KNN) can also be used to impute missing values based on the nearest samples.\n",
    "            - Fills missing values by averaging values from the K nearest neighbors in the dataset.\n",
    "    \n",
    "        - ### <span style=\"color:pink\"><b>Process</b></span>\n",
    "            - Calculates distance between instances to find K nearest neighbors.\n",
    "            - Imputes missing values using the mean (for continuous features) or mode (for categorical features) of the neighbors.\n",
    "\n",
    "        - ### <span style=\"color:pink\"><b>Benefits</b></span>\n",
    "            - Preserves local structure and relationships in the data.\n",
    "            - Simple and effective when sufficient similar observations are present.\n",
    "            \n",
    "\n",
    "        ```bash\n",
    "        from sklearn.impute import KNNImputer\n",
    "        \n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Simple Imputer</b></span>\n",
    "    \n",
    "        The `SimpleImputer` class can be used to specify different strategies for imputation.\n",
    "        \n",
    "        ```bash\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent', etc.\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        ```\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Imputing with Min/Max Values</b></span>\n",
    "    \n",
    "        You can also use the minimum or maximum values for imputation.\n",
    "        \n",
    "        ```bash\n",
    "        # Impute with minimum value\n",
    "        data.fillna(data.min(), inplace=True)\n",
    "\n",
    "        # Impute with maximum value\n",
    "        data.fillna(data.max(), inplace=True)\n",
    "        ```\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>When to Use Each Technique</b></span>\n",
    "\n",
    "    - **Mean, Median, Mode Imputation**: Use these methods for numerical features when the data is symmetrically distributed. Median is preferable for skewed distributions.\n",
    "    - **Forward Fill / Backward Fill**: Suitable for time series data where the order is important, and missing values are expected to be similar to nearby values.\n",
    "    - **Iterative Imputation (MICE)**: Best for datasets with complex relationships among features. This technique often yields better results when features are correlated.\n",
    "    - **KNN Imputation**: Effective for datasets where the values of a feature are influenced by other features. It's useful when the dataset is not too large, as it can be computationally expensive.\n",
    "    - **Simple Imputer**: Useful for general cases where a specific strategy is required. It offers flexibility in choosing the imputation strategy.\n",
    "    - **Min/Max Imputation**: Generally used for bounded features, but use with caution as it can reduce variability in the data.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Why We Need Imputation</b></span>\n",
    "\n",
    "    Imputation is crucial for maintaining the usability of a dataset, especially in real-world applications where missing values are common. By imputing missing data, we can preserve the size and integrity of the dataset, which is vital for training effective machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Impact on Actual Data and Model</b></span>\n",
    "\n",
    "    The impact of data imputation can be both positive and negative:\n",
    "    \n",
    "    - **Positive**: Imputation can lead to more robust models that generalize better due to the increased amount of usable data.\n",
    "    - **Negative**: If not done correctly, imputation can introduce bias, reduce variability, or distort relationships between features, ultimately leading to poor model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Regularization</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Regularization is a set of methods aimed at **reducing overfitting** in machine learning models. Typically, it involves trading a marginal decrease in **training accuracy** for an increase in **generalizability**â€”the model's ability to produce accurate predictions on new datasets.\n",
    "-   Basically, regularization increases a modelâ€™s generalizability but often results in **higher training error**. This means models may perform less accurately on training data while improving predictions on test data.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Bias-Variance Tradeoff</b></span>\n",
    "\n",
    "The concession of increased training error for decreased testing error is known as the **bias-variance tradeoff**. Here's a brief breakdown:\n",
    "\n",
    "- **Bias**: Measures the average difference between predicted and true values. High bias results in high error on the training set.\n",
    "  \n",
    "- **Variance**: Measures how much predictions differ across various subsets of the same data. High variance indicates poor performance on unseen data.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Key Points on Variance:</b></span>\n",
    "- **Variance** in machine learning reflects how much a model's predictions change when trained on different data subsets. It signifies a model's sensitivity to training data.\n",
    "\n",
    "- **Different Subsets, Different Models**: Training on different data subsets often results in slightly different models due to randomness.\n",
    "  \n",
    "- **Prediction Variation**: These models may produce varying predictions on unseen data, with variance measuring the extent of this variation.\n",
    "\n",
    "- **Lower Prediction Variance**: Indicates that the model generalizes well rather than memorizing patterns.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Aim of Regularization:</b></span>\n",
    "\n",
    "Developers strive to reduce both bias and variance. However, simultaneous reduction isn't always achievable, leading to the need for regularization, which decreases model variance at the cost of increased bias.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding Overfitting and Underfitting:</b></span>\n",
    "\n",
    "- **Overfitting**: \n",
    "    -   Characterized by low bias and high variance. This occurs when a model learns noise from the training data.\n",
    "    -   Happens when the model is too complex and captures even the noise in the data, making it perform well on the training data but poorly on unseen data.\n",
    "- **Underfitting**: \n",
    "    -   Refers to high bias and high variance, resulting in poor predictions on both training and test data. This often arises from insufficient training data or parameters.\n",
    "    -   Occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "---\n",
    "\n",
    "- ### <span style=\"color:#D35400\"><b>Impact of Data Size on Underfitting and Overfitting</b></span>\n",
    "\n",
    "    - #### <span style=\"color:#28B463\"><b>1. Small Data Size</b></span>\n",
    "\n",
    "        When the dataset is small, **overfitting** is more likely to occur because the model memorizes the limited data points and fails to generalize to new data.\n",
    "\n",
    "        `Small data`: Models may learn specific details (including noise) and struggle when exposed to new data.\n",
    "\n",
    "    - #### <span style=\"color:#28B463\"><b>2. Large Data Size</b></span>\n",
    "\n",
    "        With **more data**, the risk of **overfitting decreases**, as the model has a larger, more diverse set of examples to learn from. However, with a simple model, underfitting might occur because the model cannot capture the complexity of the larger dataset.\n",
    "\n",
    "---\n",
    "\n",
    "- ### <span style=\"color:#D35400\"><b>Balancing the Data and Model Complexity</b></span>\n",
    "\n",
    "    - **Larger datasets** generally help reduce overfitting because the model can generalize better. However, to prevent underfitting, **model complexity** should increase with the size of the dataset.\n",
    "\n",
    "    - Proper techniques such as **cross-validation**, **regularization**, and **model tuning** are crucial to ensuring that the model neither underfits nor overfits, regardless of the data size.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src=\"../../../images/bias_variance_tradeoff.jpg\" alt=\"error\" width=\"600\"/></center>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Regularization Effects:</b></span>\n",
    "\n",
    "While regularization aims to reduce overfitting, it can also lead to underfitting if too much bias is introduced. Thus, determining the appropriate type and degree of regularization requires careful consideration of:\n",
    "\n",
    "- Model complexity\n",
    "- Dataset characteristics\n",
    "- Specific requirements of the task\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Ridge Regression: Introducing Regularization</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Ridge Regression**, the primary difference from ordinary linear regression is the inclusion of a **regularization term**. This term helps penalize large weights to prevent **overfitting**, leading to a model that generalizes better.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Ridge Regression Loss Function</b></span>\n",
    "\n",
    "The **Ridge Regression Loss** function combines the **Mean Squared Error (MSE)** with a penalty term that controls the magnitude of the weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Ridge}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda w^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( $\\lambda$ \\) is the **regularization parameter** (often referred to as **alpha** in Ridge, which should not be confused with the learning rate \\( $\\alpha$ \\)),\n",
    "- \\( $w^2$ \\) is the **sum of the squared weights**.\n",
    "\n",
    "The additional term \\( $\\lambda w^2$ \\) discourages the model from learning large weights, which may lead to overfitting.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Ridge Loss with Respect to Weight \\( $w$ \\)</b></span>\n",
    "\n",
    "To derive the weight update rule, we need to compute the **gradient of the Ridge Loss** with respect to the weight \\( $w$ \\). This consists of two parts:\n",
    "\n",
    "- **Gradient of the MSE** (same as in linear regression):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the regularization term \\( \\lambda w^2 \\)**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda w^2}{\\partial w} = 2\\lambda w\n",
    "$$\n",
    "\n",
    "Thus, the total gradient for **Ridge Regression** becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Ridge}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + 2\\lambda w\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Ridge Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\( w \\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + 2\\lambda w_{\\text{old}} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying the update formula:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - 2\\alpha\\lambda w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "This final equation shows how the weight is updated at each step of gradient descent in **Ridge Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Key Differences from Ordinary Linear Regression</b></span>\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Regularization Term</b></span>\n",
    "\n",
    "The key difference in Ridge Regression is the inclusion of the **second term** \\( $2\\alpha\\lambda w_{\\text{old}}$ \\) in the weight update rule. This term penalizes large values of \\( $w$ \\), shrinking the weights over time and helping to prevent **overfitting**.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Regularization Parameter \\( $\\lambda$ \\)</b></span>\n",
    "\n",
    "- The **regularization parameter \\( $\\lambda$ \\)** controls the strength of the penalty. \n",
    "- When \\( $\\lambda$ = 0 \\), Ridge Regression becomes equivalent to **ordinary linear regression**. \n",
    "- A larger \\( $\\lambda$ \\) results in greater penalization, pushing the weights towards zero and reducing model complexity.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact on Generalization</b></span>\n",
    "\n",
    "The regularization term \\( $\\lambda w^2$ \\) encourages the model to have **smaller weights**, preventing it from overfitting the training data. This allows the model to generalize better to unseen data, avoiding **overly complex solutions** that fit noise in the data.\n",
    "\n",
    "---\n",
    "\n",
    "By adding **Ridge regularization**, we improve the **stability** of the linear model, especially when dealing with **multicollinearity** (where predictor variables are highly correlated). Ridge Regression is an effective tool when you need to balance between fitting your data and maintaining a model that generalizes well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Lasso Regression: Emphasizing Feature Selection</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Lasso Regression**, the key difference from ordinary linear regression is the introduction of a **regularization term** that encourages sparsity in the model. This means that some coefficients can become exactly zero, leading to a simpler and more interpretable model.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Lasso Regression Loss Function</b></span>\n",
    "\n",
    "The **Lasso Regression Loss** function integrates the **Mean Squared Error (MSE)** with a penalty term based on the absolute values of the weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Lasso}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |w_j|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\lambda $\\) is the **regularization parameter** (also called **alpha** in Lasso, which should not be confused with the learning rate \\($ \\alpha $\\)),\n",
    "- \\($ |w_j| $\\) is the **absolute sum of the weights**.\n",
    "\n",
    "The term \\($ \\lambda \\sum_{j=1}^{p} |w_j| $\\) encourages some weights to shrink to zero, effectively performing feature selection.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Lasso Loss with Respect to Weight \\($ w $\\)</b></span>\n",
    "\n",
    "To derive the weight update rule for Lasso Regression, we compute the **gradient of the Lasso Loss** with respect to the weight \\($ w $\\). The gradient consists of two components:\n",
    "\n",
    "- **Gradient of the MSE** (same as in linear regression):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the regularization term \\($ \\lambda |w| $\\)**:\n",
    "\n",
    "The derivative with respect to \\($ w $\\) involves the **sign function**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda |w|}{\\partial w} = \\lambda \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "So, the total gradient for **Lasso Regression** is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Lasso}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Lasso Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\($ w $\\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) \\right)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - \\alpha \\lambda \\cdot \\text{sgn}(w_{\\text{old}})\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. Key Differences from Ordinary Linear Regression</b></span>\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Regularization Term</b></span>\n",
    "\n",
    "The major distinction in Lasso Regression is the inclusion of the **absolute value term** \\( \\alpha \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) \\) in the weight update rule. This term can drive some weights exactly to zero, allowing the model to exclude less important features.\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Regularization Parameter \\( \\lambda \\)</b></span>\n",
    "\n",
    "- The **regularization parameter \\($ \\lambda $\\)** controls the strength of the penalty.\n",
    "- When \\($ \\lambda = 0 $\\), Lasso Regression becomes equivalent to **ordinary linear regression**.\n",
    "- A larger \\($ \\lambda $\\) increases the penalty, promoting more weights to become zero and leading to a simpler model.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Weight Updates in Lasso Regression</b></span>\n",
    "\n",
    "In Lasso Regression, the weight update rule incorporates the term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\). This term plays a critical role in how the weights are adjusted during training. Let's break down its impact:\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding the Penalty Term</b></span>\n",
    "\n",
    "- **\\($ \\lambda $\\)**: This is the regularization parameter that controls the strength of the penalty. A larger \\($ \\lambda $\\) encourages more weights to shrink towards zero.\n",
    "  \n",
    "- **\\($ \\text{sgn}(w_{\\text{old}})$\\)**: The sign function returns:\n",
    "  - **1** if \\($ w_{\\text{old}} > 0 $\\) (positive weight)\n",
    "  - **-1** if \\($ w_{\\text{old}} < 0 $\\) (negative weight)\n",
    "  - **0** if \\($ w_{\\text{old}} = 0 $\\) (zero weight)\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact of the Penalty Term</b></span>\n",
    "\n",
    "1. **When \\($ w_{\\text{old}} $\\) is Positive**:\n",
    "   - The term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\) contributes positively to the weight update.\n",
    "   - **Effect**: The penalty reduces the value of the weight \\($ w_{\\text{new}} $\\). \n",
    "   - **Interpretation**: This encourages the weight to shrink, thus regularizing the model.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left(\\text{penalty}\\right) \\quad \\text{(penalty is positive)}\n",
    "   $$\n",
    "\n",
    "2. **When \\($ w_{\\text{old}} $\\) is Negative**:\n",
    "   - The term \\($ \\lambda \\cdot \\text{sgn}(w_{\\text{old}}) $\\) contributes negatively to the weight update.\n",
    "   - **Effect**: The penalty increases the value of the weight \\($ w_{\\text{new}} $\\) (making it less negative).\n",
    "   - **Interpretation**: This adjustment reduces the magnitude of the negative weight, pushing it closer to zero.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\left(\\text{penalty}\\right) \\quad \\text{(penalty is negative)}\n",
    "   $$\n",
    "\n",
    "3. **When \\($ w_{\\text{old}} $\\) is Small (Close to Zero)**:\n",
    "   - If \\($ |w_{\\text{old}}| $\\) is small enough, the penalty can effectively drive the weight to exactly zero.\n",
    "   - **Effect**: The weight \\($ w_{\\text{new}} $\\) becomes zero, effectively eliminating that feature from the model.\n",
    "   - **Interpretation**: This feature selection property is a key benefit of Lasso Regression.\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = 0 \\quad \\text{(if the update drives \\( w_{\\text{old}} \\) to zero)}\n",
    "   $$\n",
    "\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Impact on Feature Selection</b></span>\n",
    "\n",
    "The L1 penalty encourages sparsity, meaning that Lasso can eliminate irrelevant features entirely by setting their corresponding weights to zero. This makes Lasso an effective method for feature selection in high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:#2E86C1\"><b>Elastic Net Regression:(Combination of Ridge and Lasso)</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net Regression** combines both Lasso and Ridge regression to achieve a balance between feature selection and regularization. It is particularly useful when dealing with highly correlated features.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. Elastic Net Loss Function</b></span>\n",
    "\n",
    "The **Elastic Net Loss** function integrates the **Mean Squared Error (MSE)** with both L1 and L2 penalty terms:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{Elastic Net}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\left( l_1 \\sum_{j=1}^{p} |w_j| + (1 - l_1) \\sum_{j=1}^{p} w_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\($ \\lambda $\\) is the **regularization parameter** (similar to Lasso).\n",
    "- \\($ l_1 $\\) is the **l1_ratio**, which controls the balance between Lasso and Ridge penalties (0 â‰¤ l1_ratio â‰¤ 1).\n",
    "- \\($ |w_j| $\\) is the **absolute sum of the weights** (L1 penalty).\n",
    "- \\($ w_j^2 $\\) is the **sum of squares of the weights** (L2 penalty).\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Gradient of Elastic Net Loss with Respect to Weight \\($ w $\\)</b></span>\n",
    "\n",
    "To derive the weight update rule for Elastic Net Regression, we compute the **gradient of the Elastic Net Loss** with respect to the weight \\($ w $\\). The gradient consists of three components:\n",
    "\n",
    "- **Gradient of the MSE**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{MSE}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the L1 regularization term** (Lasso):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\lambda l_1 \\sum_{j=1}^{p} |w|)}{\\partial w} = \\lambda l_1 \\cdot \\text{sgn}(w)\n",
    "$$\n",
    "\n",
    "- **Gradient of the L2 regularization term** (Ridge):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\lambda (1 - l_1) \\sum_{j=1}^{p} w^2\\right)}{\\partial w} = 2\\lambda (1 - l_1) w\n",
    "$$\n",
    "\n",
    "The total gradient for **Elastic Net Regression** is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}_{\\text{Elastic Net}}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda l_1 \\cdot \\text{sgn}(w) + 2\\lambda (1 - l_1) w\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Weight Update Rule for Elastic Net Regression</b></span>\n",
    "\n",
    "Using the **gradient descent** algorithm, we update the weight \\($ w $\\) based on the computed gradient:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\left( -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) + \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) + 2\\lambda (1 - l_1) w_{\\text{old}} \\right)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} + \\frac{2\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) - \\alpha \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) - 2\\alpha \\lambda (1 - l_1) w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. Key Differences from Lasso and Ridge Regression</b></span>\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>Combination of Penalties</b></span>\n",
    "\n",
    "- Elastic Net combines L1 and L2 penalties, allowing it to benefit from both feature selection (L1) and regularization (L2).\n",
    "\n",
    "#### <span style=\"color:#28B463\"><b>l1_ratio Parameter</b></span>\n",
    "\n",
    "- The **l1_ratio** parameter controls the balance between Lasso and Ridge regularization:\n",
    "  - If \\($ l_1 = 1 $\\), Elastic Net behaves like Lasso.\n",
    "  - If \\($ l_1 = 0 $\\), Elastic Net behaves like Ridge.\n",
    "  - Values between 0 and 1 provide a mix of both.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Weight Updates in Elastic Net Regression</b></span>\n",
    "\n",
    "In Elastic Net Regression, the weight update rule incorporates both L1 and L2 regularization terms, balanced by the **l1_ratio**. Let's break down the impacts:\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Understanding the Components</b></span>\n",
    "\n",
    "1. **L1 Regularization Term**:\n",
    "   - The term \\($ \\lambda l_1 \\cdot \\text{sgn}(w_{\\text{old}}) $\\) reduces the weights.\n",
    "  \n",
    "2. **L2 Regularization Term**:\n",
    "   - The term \\($ 2\\lambda (1 - l_1) w_{\\text{old}} $\\) penalizes larger weights, encouraging weight decay.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Impact on Weight Updates</b></span>\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Positive**:\n",
    "  - The L1 term reduces the weight, while the L2 term further encourages smaller weights.\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Negative**:\n",
    "  - The L1 term increases the weight, pushing it closer to zero, while the L2 term counteracts by promoting decay.\n",
    "\n",
    "- **When \\($ w_{\\text{old}} $\\) is Small (Close to Zero)**:\n",
    "  - Both regularization terms work together to drive the weight towards zero, allowing for effective feature selection.\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Overall Effect on Feature Selection</b></span>\n",
    "\n",
    "The Elastic Net regression encourages sparsity and feature selection while retaining some ability to handle correlated features due to the inclusion of the L2 penalty. This makes it an effective choice in scenarios where there are many features, some of which may be highly correlated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Regularization in Deep Learning</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:#D35400\"><b>4. Dropout Regularization (Neural Networks)</b></span>\n",
    "\n",
    "- **Explanation**: **Dropout** is a regularization technique primarily used in neural networks. During training, randomly selected neurons are \"dropped\" or ignored, preventing the model from becoming too dependent on particular neurons and reducing overfitting.\n",
    "  \n",
    "- **How It Works**: Neurons are randomly set to zero during each training step, which forces the model to learn more robust representations.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model.add(Dropout(0.5))\n",
    "```\n",
    "\n",
    "- **Use Case**: Especially useful in **deep learning models** to prevent overfitting, particularly in large networks.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>5. Early Stopping</b></span>\n",
    "\n",
    "- **Explanation**: **Early stopping** halts the training process when the performance on a validation dataset starts to degrade. This prevents the model from continuing to fit the noise in the training data.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "- **Use Case**: Commonly used in **deep learning** to reduce overfitting when training for a large number of epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>6. Data Augmentation (Deep Learning)</b></span>\n",
    "\n",
    "- **Explanation**: **Data Augmentation** increases the size of the training dataset by applying transformations (rotations, flips, etc.) to existing data. Itâ€™s a form of regularization that forces the model to learn more robust features by exposing it to slightly varied data.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "```\n",
    "\n",
    "- **Use Case**: Especially effective in **computer vision** tasks when training datasets are small.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>7. Weight Regularization (Neural Networks)</b></span>\n",
    "\n",
    "- **Explanation**: In neural networks, **weight regularization** techniques (like L1 or L2 penalties) are applied to the weights of the network to limit their size, thus preventing overfitting.\n",
    "\n",
    "- **Sample Code** (Keras):\n",
    "```bash\n",
    "from tensorflow.keras.regularizers import l2\n",
    "model.add(Dense(units=64, kernel_regularizer=l2(0.01)))\n",
    "```\n",
    "\n",
    "- **Use Case**: Applied in deep learning networks to control the size of weights and avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "By understanding and applying the right type of regularization, you can control the complexity of your machine learning models, prevent overfitting, and improve generalization on unseen data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Parameter Tuning with GridSearchCV and RandomizedSearchCV</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## <span style=\"color:#D35400\"><b>What is Parameter Tuning?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Definition:</b></span>\n",
    "        - **Parameter tuning** refers to finding the optimal set of **hyperparameters** (parameters that are not learned from the data, like `learning rate`, `regularization strength`, etc.) for a machine learning model.\n",
    "        - It improves the modelâ€™s performance by choosing values that lead to better predictions on unseen data.\n",
    "        - **Example**: In a Support Vector Machine (SVM), hyperparameters like `C` (regularization parameter) and `kernel type` need tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>What is GridSearchCV?</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Definition:</b></span> \n",
    "    - It is an **exhaustive search** tool that automates hyperparameter tuning. \n",
    "    - It tries all combinations of the parameters you provide to find the **best set** for the model.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>How Does GridSearchCV Work?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Process:</b></span>\n",
    "        1. **Define a parameter grid**: You list out multiple values for each hyperparameter that you want to test.\n",
    "        2. **Cross-validation**: It uses **cross-validation** (like `K-Fold`) to test each combination of hyperparameters on different data splits.\n",
    "        3. **Best parameters**: After evaluating all combinations, it selects the **best set** of parameters based on performance (like accuracy, precision).\n",
    "    - **Key Point**: GridSearchCV evaluates multiple combinations of hyperparameters, ensuring you find the **optimal configuration** for your model.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Why Use GridSearchCV?</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Purpose:</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Optimization:</b></span>\n",
    "        - It allows you to optimize your model by **automating the search** for the best hyperparameter values, making your model more **accurate**.\n",
    "    - ### <span style=\"color:#28B463\"><b>Efficiency:</b></span>\n",
    "        - Itâ€™s a **systematic** way of trying every combination instead of manually testing each hyperparameter value, which saves time and effort.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>How to Implement GridSearchCV?</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Steps:</b></span>\n",
    "    1. **Import GridSearchCV** from `sklearn.model_selection`.\n",
    "    2. **Define your parameter grid**: Specify a dictionary where keys are the hyperparameters and values are lists of possible values.\n",
    "    3. **Fit the model**: Run GridSearchCV to find the best parameters based on cross-validation.\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Example Code:</b></span>\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.model_selection import GridSearchCV , StratifiedKFold\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Define the model\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "\n",
    "    #Cross-validation \n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, verbose=2, scoring='accuracy')\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Benefits of GridSearchCV</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Key Advantages:</b></span>\n",
    "    - **Automation**: It saves time by automatically testing multiple parameter combinations.\n",
    "    - **Cross-validation**: Ensures robust evaluation by using cross-validation for each parameter set.\n",
    "    - **Improved Performance**: Finds the combination that makes the model more **accurate** and **generalizable**.\n",
    "\n",
    "--- \n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>What is RandomizedSearchCV?</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Definition:</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>RandomizedSearchCV</b></span> \n",
    "    - It is a **hyperparameter tuning technique** that randomly samples a fixed number of parameter combinations from a grid, rather than trying every possible combination like **GridSearchCV**.\n",
    "    - Itâ€™s more **efficient** when the hyperparameter space is large and you need a quicker search.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>How Does RandomizedSearchCV Work?</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Process:</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Steps:</b></span>\n",
    "        1. **Define a parameter distribution**: Instead of listing all values, you define a **range** for each hyperparameter.\n",
    "        2. **Random sampling**: RandomizedSearchCV **randomly selects** a set number of hyperparameter combinations from the defined ranges.\n",
    "        3. **Cross-validation**: Like GridSearchCV, it uses **cross-validation** to evaluate the performance of each combination.\n",
    "        4. **Best parameters**: After testing a fixed number of random combinations, it returns the **best-performing** set of parameters.\n",
    "    - **Key Point**: It **reduces computation time** by testing fewer combinations, making it faster for large parameter spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Why Use RandomizedSearchCV?</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Purpose:</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Efficiency:</b></span>\n",
    "        - Itâ€™s useful when the parameter space is **large**, and you donâ€™t want to exhaustively try every possible combination.\n",
    "    - ### <span style=\"color:#28B463\"><b>Speed:</b></span>\n",
    "        - By **sampling** random combinations, it speeds up the hyperparameter search process compared to GridSearchCV.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>How to Implement RandomizedSearchCV?</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Steps:</b></span>\n",
    "    1. **Import RandomizedSearchCV** from `sklearn.model_selection`.\n",
    "    2. **Define your parameter distributions**: Use ranges for the parameters instead of lists of values.\n",
    "    3. **Run the search**: Use RandomizedSearchCV to sample parameter combinations and find the best one.\n",
    "\n",
    "    - ### <span style=\"color:#28B463\"><b>Example Code:</b></span>\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.model_selection import RandomizedSearchCV , StratifiedKFold\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import numpy as np\n",
    "\n",
    "    # Define the model\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Define the parameter distribution\n",
    "    param_dist = {\n",
    "        'n_estimators': np.arange(50, 200, 10),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': np.arange(2, 11)\n",
    "    }\n",
    "\n",
    "    #Cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Set up RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=kfold, verbose=2, scoring='accuracy')\n",
    "\n",
    "    # Fit the model\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters\n",
    "    print(\"Best Parameters: \", random_search.best_params_)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Benefits of RandomizedSearchCV</b></span>\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Key Advantages:</b></span>\n",
    "    - **Faster Search**: Itâ€™s faster than GridSearchCV because it doesnâ€™t evaluate all parameter combinations.\n",
    "    - **Efficient for Large Spaces**: Ideal when the parameter space is large, as it **randomly samples** a subset of combinations.\n",
    "    - **Balanced Accuracy**: While it may not guarantee the absolute best parameters, it finds a close-to-optimal solution with less computation. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Train, Test, and Validation Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## <span style=\"color:#D35400\"><b>What is Training Data?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Definition:</b></span>\n",
    "        - **Training Data** is the part of the dataset that the model **learns from**.\n",
    "        - It consists of both input features and the corresponding output labels.\n",
    "        - The model identifies patterns in this data, adjusting its internal parameters (like `weights` and `biases`) to minimize errors.\n",
    "        - **Example**: In a housing price prediction model, the training data would include house features (size, location) as inputs and actual house prices as output labels.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>What is Test Data?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Definition:</b></span>\n",
    "        - **Test Data** is used to **evaluate the model's performance** after training.\n",
    "        - It provides unseen examples that the model has not encountered during training, helping to assess how well the model **generalizes** to new data.\n",
    "        - **Key Point**: The model should not be trained on the test data, as its purpose is to simulate real-world performance.\n",
    "        - **Example**: After training the housing price model, we use test data (with unseen houses) to check how accurately the model predicts their prices.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>What is Validation Data?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Definition:</b></span>\n",
    "        - **Validation Data** is a subset of the data used during training to **tune hyperparameters** and check the model's performance as it trains.\n",
    "        - It helps in decisions like choosing the **optimal number of layers** in a neural network or the **best regularization technique**.\n",
    "        - The model uses this data to check for `overfitting` or `underfitting` during training, but it does not learn from it directly.\n",
    "        - **Example**: While training the housing price model, you might test it on validation data every few epochs to see if the model is improving and adjusting hyperparameters accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "```bash\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,shuffle=True,random_state=23,stratify=y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Cross-Validation</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## <span style=\"color:#D35400\"><b>What is Cross-Validation?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Definition:</b></span>\n",
    "        - **Cross-Validation** is a method used to further evaluate the model by **splitting the data into smaller subsets** (called `folds`).\n",
    "        - **K-Fold Cross-Validation** is the most commonly used method.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>How Does K-Fold Cross-Validation Work?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Process:</b></span>\n",
    "        1. The data is split into **K subsets** (folds).\n",
    "        2. The model is trained on **K-1 folds** and tested on the **remaining fold**.\n",
    "        3. This process is repeated **K times**, where each fold serves as the test set once, and the rest as the training set.\n",
    "        4. The **average performance** across all folds gives an overall measure of the model's ability to generalize.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Why Use Cross-Validation?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Purpose:</b></span>\n",
    "        - It **checks for overfitting** (whether the model works well on the training data but poorly on unseen data).\n",
    "        - It ensures that the model is not simply learning to **memorize** the training data but is **generalizing** well to new, unseen data.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Example:</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Illustration:</b></span>\n",
    "        - If you have a dataset of 1,000 housing prices, and you use **5-fold cross-validation**, you would split the dataset into 5 parts:\n",
    "            - Train on 4 parts (80%) and test on the remaining 1 part (20%).\n",
    "            - Repeat the process, rotating which part is the test set each time.\n",
    "            - In the end, you get 5 different evaluations of the model, ensuring it's performing well across various parts of the data.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center><span style=\"color:#28B463\"><b>This is K-fold Cross Validation</b></span></center>\n",
    "\n",
    "<center><img src=\"../../../images/kfoldcrossvalidation.png\" alt=\"error\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center><span style=\"color:#28B463\"><b>This is Stratified K-fold Cross Validation</b></span></center>\n",
    "<center><img src=\"../../../images/stratifiedkfoldcrossvalidation.png\" alt=\"error\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Stratification</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## <span style=\"color:#D35400\"><b>What is Stratification?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Definition:</b></span>\n",
    "        - **Stratification** ensures that the **class proportions** in the training, validation, and test sets are the same as in the original dataset.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Why Use Stratification?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Purpose:</b></span>\n",
    "        - It's especially important for **imbalanced datasets**, where certain classes are underrepresented.\n",
    "        - Without stratification, there is a risk that some subsets might not include enough examples of the minority class, leading to poor model performance on those classes.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>Example:</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Illustration:</b></span>\n",
    "        - In a dataset where 90% of the houses are in an urban area and only 10% in a rural area, **stratified sampling** ensures that both the training and test sets maintain this ratio, allowing the model to properly learn from both urban and rural data.\n",
    "\n",
    "- ## <span style=\"color:#D35400\"><b>How to Implement Stratification?</b></span>\n",
    "    - ### <span style=\"color:#28B463\"><b>Tip:</b></span>\n",
    "        - In `scikit-learn`, when using `train_test_split`, you can stratify your data using the `stratify` parameter to ensure the class proportions are consistent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Data Encoding</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#D35400\"><b>1. One-Hot Encoding</b></span>\n",
    "- ### <span style=\"color:#28B463\"><b>Description</b></span>\n",
    "  - Converts categorical variables into binary (0 or 1) columns for each category. This approach ensures that no **ordinal relationships** are implied between the categories.\n",
    "  \n",
    "- ### <span style=\"color:#28B463\"><b>Example</b></span>\n",
    "  - Suppose we have a categorical feature **Color** with three categories: **Red**, **Green**, and **Blue**.\n",
    "\n",
    "  | Color (Original) | Red | Green | Blue |\n",
    "  |------------------|-----|-------|------|\n",
    "  | Red              | 1   | 0     | 0    |\n",
    "  | Green            | 0   | 1     | 0    |\n",
    "  | Blue             | 0   | 0     | 1    |\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>When to Use</b></span>\n",
    "  - Use One-Hot Encoding for **nominal categorical variables** where there is no inherent order, such as colors, types of animals, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>2. Label Encoding</b></span>\n",
    "- ### <span style=\"color:#28B463\"><b>Description</b></span>\n",
    "  - Converts each category into a unique integer. It is suitable for **ordinal categorical variables** where order matters.\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>Example</b></span>\n",
    "  - Consider the ordinal feature **Size** with categories: **Small**, **Medium**, and **Large**.\n",
    "\n",
    "  | Size (Original) | Size (Encoded) |\n",
    "  |------------------|----------------|\n",
    "  | Small            | 0              |\n",
    "  | Medium           | 1              |\n",
    "  | Large            | 2              |\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>When to Use</b></span>\n",
    "  - Use Label Encoding for **ordinal categorical variables** where the order is significant, such as ratings or sizes.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>3. Binary Encoding</b></span>\n",
    "- ### <span style=\"color:#28B463\"><b>Description</b></span>\n",
    "  - Converts categories into binary format, reducing dimensionality. Each category is represented as a binary number.\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>Example</b></span>\n",
    "  - For a feature with categories: **Cat**, **Dog**, and **Fish**.\n",
    "\n",
    "  | Animal (Original) | Animal (Binary) |\n",
    "  |--------------------|------------------|\n",
    "  | Cat                | 00               |\n",
    "  | Dog                | 01               |\n",
    "  | Fish               | 10               |\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>When to Use</b></span>\n",
    "  - Use Binary Encoding for **high-cardinality features** to reduce dimensionality compared to One-Hot Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>4. Target Encoding ( Mean Encoding )</b></span>\n",
    "- ### <span style=\"color:#28B463\"><b>Description</b></span>\n",
    "  - Replaces each category with the mean of the target variable for that category. This approach captures the relationship between the **categorical feature** and the target variable.\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>Example</b></span>\n",
    "  - For a feature **City** and a target variable **House Price**:\n",
    "\n",
    "  | City | House Price | Encoded Value |\n",
    "  |------|-------------|----------------|\n",
    "  | A    | 200,000     | 210,000        |\n",
    "  | B    | 250,000     | 250,000        |\n",
    "  | A    | 220,000     | 210,000        |\n",
    "  | C    | 300,000     | 300,000        |\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>When to Use</b></span>\n",
    "  - Use Target Encoding when you have a **strong relationship** between the categorical feature and the target variable. Be cautious of **overfitting** and consider using cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>5. Frequency Encoding ( Count Encoding )</b></span>\n",
    "- ### <span style=\"color:#28B463\"><b>Description</b></span>\n",
    "  - Replaces each category with its **frequency** in the dataset. This approach captures how common each category is.\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>Example</b></span>\n",
    "  - For a feature **Product Type**:\n",
    "\n",
    "    | Product Type  |   \n",
    "    | ------------- | \n",
    "    | A             |\n",
    "    | B             |\n",
    "    | A             |\n",
    "    | C             |\n",
    "    | B             |\n",
    "    | A             |\n",
    "\n",
    "- **Encodings**:\n",
    "\n",
    "  | Product Type | Frequency |\n",
    "  |--------------|-----------|\n",
    "  | A            | 3         |\n",
    "  | B            | 2         |\n",
    "  | C            | 1         |\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>When to Use</b></span>\n",
    "  - Use Frequency Encoding for **high-cardinality categorical features** where the frequency of occurrence can provide meaningful information.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Data Pipelining and Data Transformation</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:#2E86C1\"><b>What is a Pipeline?</b></span>\n",
    "\n",
    "- **Pipeline** is a tool that **sequentially applies** multiple steps (such as data preprocessing and model training) in a machine learning workflow. \n",
    "- It ensures that each step is executed in the correct order and can include data transformations, feature engineering, and model fitting in a single object.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#2E86C1\"><b>Why Use a Pipeline?</b></span>\n",
    "\n",
    "- <span style=\"color:#28B463\"><b>Automation</b></span>: It **automates** the entire machine learning workflow, from preprocessing to model training.\n",
    "- <span style=\"color:#28B463\"><b>Consistency</b></span>: Ensures that transformations on training and test data are consistent, avoiding data leakage.\n",
    "- <span style=\"color:#28B463\"><b>Efficiency</b></span>: Reduces code redundancy by chaining preprocessing and model steps together.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>How to Implement a Pipeline?</b></span>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Steps:</b></span>\n",
    "\n",
    "1. **Import Pipeline** from `sklearn.pipeline`.\n",
    "2. **Define steps**: Create a list of tuples where each tuple contains a name and the corresponding transformation/model.\n",
    "3. **Fit and predict**: The pipeline can be fit and used for predictions as a single unit.\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>Example Code:</b></span>\n",
    "\n",
    "```bash\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),   # Step 1: Scale the features\n",
    "    ('rf', RandomForestClassifier()) # Step 2: Train the model\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "predictions = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>What is a ColumnTransformer?</b></span>\n",
    "\n",
    "- **ColumnTransformer** allows you to **apply different transformations to different columns** of a dataframe.\n",
    "- Itâ€™s particularly useful when you have a combination of **numerical** and **categorical** features that require different preprocessing techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#2E86C1\"><b>Why Use ColumnTransformer?</b></span>\n",
    "\n",
    "- <span style=\"color:#28B463\"><b>Customized Transformations</b></span>: You can apply specific transformations (like `StandardScaler` for numerical features and `OneHotEncoder` for categorical features) to individual columns.\n",
    "- <span style=\"color:#28B463\"><b>Efficiency</b></span>: Avoids redundant transformations by directly applying preprocessing to **only** the relevant columns.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>How to Implement ColumnTransformer?</b></span>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Steps:</b></span>\n",
    "\n",
    "1. **Import ColumnTransformer** from `sklearn.compose`.\n",
    "2. **Define transformers**: Specify a list of tuples containing the name, transformation, and columns to apply it to.\n",
    "3. **Fit and transform**: Use the transformer on your dataset, applying the transformations to the appropriate columns.\n",
    "\n",
    "- ### <span style=\"color:#28B463\"><b>Example Code:</b></span>\n",
    "\n",
    "```bash\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the transformers\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['Age', 'Salary']),       # Scale numerical columns\n",
    "        ('cat', OneHotEncoder(), ['Gender', 'Country'])     # One-hot encode categorical columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the ColumnTransformer with a model in a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', column_transformer),                   # Step 1: Transform the data\n",
    "    ('rf', RandomForestClassifier())                       # Step 2: Train the model\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "predictions = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>When to Use Pipeline and ColumnTransformer?</b></span>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Pipeline Use Cases:</b></span>\n",
    "\n",
    "- **End-to-End Automation**: When you want to automate the entire process of data preprocessing, feature engineering, and model training.\n",
    "- **Consistency**: When you want to ensure that both training and test data are preprocessed in the exact same way.\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>ColumnTransformer Use Cases:</b></span>\n",
    "\n",
    "- **Mixed Data Types**: When you have a dataset with both **numerical** and **categorical** features, requiring different preprocessing steps for each type.\n",
    "- **Efficient Preprocessing**: When you want to apply specific transformations to different columns without repeating the transformation logic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Feature Selection</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of choosing a subset of relevant features (input variables) for building a machine learning model. By selecting the most important features, we can reduce overfitting, improve model performance, and decrease computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>Why Feature Selection is Important?</b></span>\n",
    "\n",
    "- **Reduces Overfitting**: Removing irrelevant or redundant features can prevent the model from learning noise.\n",
    "- **Improves Accuracy**: A more focused set of features helps the model generalize better to unseen data.\n",
    "- **Increases Efficiency**: Reducing the number of features decreases the time and resources needed for model training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Feature Selection Techniques</b></span>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Recursive Feature Elimination (RFE)</b></span>\n",
    "\n",
    "- **Definition**: `RFE` is a backward selection technique where features are recursively removed one by one, and the model is refit each time, until the optimal number of features is selected.\n",
    "  \n",
    "    - **Steps**:\n",
    "        1. Train the model with all features.\n",
    "        2. Rank the features based on their importance.\n",
    "        3. Recursively remove the least important feature and refit the model.\n",
    "        4. Repeat until the desired number of features is selected.\n",
    "\n",
    "    - **When to Use**: \n",
    "        - Use when you have a large number of features and want to find the most relevant subset.\n",
    "        - Works well for linear models and tree-based models.\n",
    "\n",
    "    - **Example Code**:\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Define the model\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Recursive Feature Elimination\n",
    "    rfe = RFE(estimator=model, n_features_to_select=5)\n",
    "    rfe.fit(X_train, y_train)\n",
    "\n",
    "    # Get selected features\n",
    "    selected_features = rfe.support_\n",
    "    print(\"Selected Features: \", selected_features)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Other Methods Include</b></span>\n",
    "-   Tree Based Models ( Random Forest )\n",
    "-   L1 Regularization ( Lasso Regresson )\n",
    "-   Univariate Selection - ( Chi test , Anova Test) \n",
    "-   Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2E86C1\"><b>Imbalanced Data Handling</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data occurs when one class (or multiple classes) is underrepresented compared to other classes in a dataset. For instance, in a classification problem with two classes, if 90% of the data points belong to one class and only 10% to the other, the dataset is imbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>Why is Handling Imbalanced Data Important?</b></span>\n",
    "\n",
    "- **Bias Towards Majority Class**: Machine learning algorithms tend to perform better on the majority class, leading to poor performance on the minority class.\n",
    "- **Poor Model Evaluation**: Accuracy can be misleading as the model might predict the majority class well but fail on the minority class.\n",
    "- **Real-World Scenarios**: Many real-world problems such as fraud detection, medical diagnosis, and rare event prediction involve imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Techniques for Handling Imbalanced Data</b></span>\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>1. SMOTE (Synthetic Minority Over-sampling Technique)</b></span>\n",
    "\n",
    "- **Definition**: SMOTE is a method to artificially generate synthetic data points for the minority class. It creates new instances by interpolating between existing minority class instances.\n",
    "  \n",
    "    - **How It Works**:\n",
    "        1. Select a data point from the minority class.\n",
    "        2. Identify its nearest neighbors.\n",
    "        3. Generate synthetic points along the line between the data point and its neighbors.\n",
    "\n",
    "    - **Advantages**:\n",
    "        - Increases the representation of the minority class without simply duplicating instances.\n",
    "        - Can help prevent overfitting compared to naive oversampling.\n",
    "\n",
    "    - **When to Use**: \n",
    "        - Use when the dataset is highly imbalanced, and the minority class needs to be expanded without introducing duplicates.\n",
    "\n",
    "    - **Example Code**:\n",
    "\n",
    "    ```bash\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # Apply SMOTE to training data\n",
    "    smote = SMOTE()\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"Before SMOTE: \", y_train.value_counts())\n",
    "    print(\"After SMOTE: \", y_resampled.value_counts())\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>2. Undersampling</b></span>\n",
    "\n",
    "- **Definition**: Undersampling reduces the number of instances in the majority class to balance the dataset with the minority class by randomly sampling from the majority class.\n",
    "  \n",
    "    - **How It Works**:\n",
    "        1. Randomly select a subset of the majority class to match the size of the minority class.\n",
    "        2. Train the model on this reduced dataset.\n",
    "\n",
    "    - **Advantages**:\n",
    "        - Simple and effective for balancing the dataset.\n",
    "        - Reduces training time by working on a smaller dataset.\n",
    "\n",
    "    - **Disadvantages**:\n",
    "        - Potential loss of important data from the majority class.\n",
    "        - Could lead to underfitting as the model has less data to learn from.\n",
    "\n",
    "    - **When to Use**: \n",
    "        - Use when the dataset is large and the loss of some majority class instances does not significantly impact model performance.\n",
    "\n",
    "    - **Example Code**:\n",
    "\n",
    "    ```bash\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    # Define undersampling strategy\n",
    "    undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "    # Apply undersampling to training data\n",
    "    X_resampled, y_resampled = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"Before Undersampling: \", y_train.value_counts())\n",
    "    print(\"After Undersampling: \", y_resampled.value_counts())\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>3. Oversampling</b></span>\n",
    "\n",
    "- **Definition**: Oversampling increases the number of instances in the minority class by duplicating existing samples or generating new samples.\n",
    "  \n",
    "    - **How It Works**:\n",
    "        1. Duplicate minority class instances until the class distributions are balanced.\n",
    "\n",
    "    - **Advantages**:\n",
    "        - Simple to implement.\n",
    "        - Ensures that the model has enough data to learn from for the minority class.\n",
    "\n",
    "    - **Disadvantages**:\n",
    "        - Risk of overfitting since the duplicated instances do not add new information.\n",
    "        - Increases training time as the dataset size grows.\n",
    "\n",
    "    - **When to Use**: \n",
    "        - Use when you want to expand the minority class with exact duplicates.\n",
    "\n",
    "    - **Example Code**:\n",
    "\n",
    "    ```bash\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "    # Define oversampling strategy\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "    # Apply oversampling to training data\n",
    "    X_resampled, y_resampled = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"Before Oversampling: \", y_train.value_counts())\n",
    "    print(\"After Oversampling: \", y_resampled.value_counts())\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>4. NearMiss (Undersampling Technique)</b></span>\n",
    "\n",
    "- **Definition**: NearMiss is an undersampling technique that selects majority class samples which are closest to the minority class. It helps retain useful majority class instances by focusing on those that are most informative.\n",
    "  \n",
    "    - **How It Works**:\n",
    "        1. For each minority class instance, select majority class instances that are closest based on distance.\n",
    "        2. Reduce the majority class using these selected samples.\n",
    "\n",
    "    - **Advantages**:\n",
    "        - Helps retain important majority class samples close to the decision boundary.\n",
    "        - Reduces the risk of losing critical information.\n",
    "\n",
    "    - **Disadvantages**:\n",
    "        - Can still lead to underfitting as it reduces the dataset size.\n",
    "\n",
    "    - **When to Use**: \n",
    "        - Use when the dataset is highly imbalanced, but removing random majority samples would result in poor model performance.\n",
    "\n",
    "    - **Example Code**:\n",
    "\n",
    "    ```bash\n",
    "    from imblearn.under_sampling import NearMiss\n",
    "\n",
    "    # Apply NearMiss to balance the dataset\n",
    "    near_miss = NearMiss()\n",
    "    X_resampled, y_resampled = near_miss.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"Before NearMiss: \", y_train.value_counts())\n",
    "    print(\"After NearMiss: \", y_resampled.value_counts())\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>5. Balanced Class Weights</b></span>\n",
    "\n",
    "- **Definition**: Some machine learning models, like logistic regression and decision trees, allow you to set class weights to handle imbalance. By assigning a higher weight to the minority class, the model pays more attention to it during training.\n",
    "  \n",
    "    - **How It Works**:\n",
    "        1. Adjust the class weights inversely proportional to the class frequencies.\n",
    "        2. The model gives more importance to the minority class during training.\n",
    "\n",
    "    - **Advantages**:\n",
    "        - Simple to implement in models that support it.\n",
    "        - Does not alter the dataset itself, so no risk of data duplication or reduction.\n",
    "\n",
    "    - **Disadvantages**:\n",
    "        - May not be as effective when the imbalance is extreme.\n",
    "\n",
    "    - **When to Use**: \n",
    "        - Use when you want the model to account for class imbalance during training without modifying the dataset.\n",
    "\n",
    "    - **Example Code**:\n",
    "\n",
    "    ```bash\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Define the model with balanced class weights\n",
    "    model = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>6. Ensemble Methods</b></span>\n",
    "\n",
    "- **Definition**: Ensemble methods like `BalancedRandomForest` and `EasyEnsemble` create balanced models by either resampling the dataset or training multiple models on balanced subsets.\n",
    "\n",
    "    - **BalancedRandomForest**: A variant of the Random Forest where each decision tree is trained on a balanced dataset using undersampling.\n",
    "    - **EasyEnsemble**: Trains multiple classifiers on different balanced subsets of the data created via undersampling.\n",
    "\n",
    "    - **Advantages**:\n",
    "        - Can boost the modelâ€™s performance on imbalanced datasets.\n",
    "        - Leverages the power of multiple models for more robust predictions.\n",
    "\n",
    "    - **Disadvantages**:\n",
    "        - More computationally expensive compared to a single model.\n",
    "        - May require tuning to avoid overfitting or underfitting.\n",
    "\n",
    "    - **Example Code**:\n",
    "\n",
    "    ```bash\n",
    "    from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "    # Define the model\n",
    "    model = BalancedRandomForestClassifier()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#D35400\"><b>Comparison of Techniques for Handling Imbalanced Data</b></span>\n",
    "\n",
    "| Technique                | When to Use                                      | Key Strength                                      | Example Use Case                      |\n",
    "|--------------------------|--------------------------------------------------|--------------------------------------------------|---------------------------------------|\n",
    "| **SMOTE**                | When you need to generate synthetic samples      | Balances data without duplicating samples         | Fraud detection, minority class expansion |\n",
    "| **Undersampling**         | When the dataset is large and majority class is too dominant | Reduces dataset size and training time            | Customer churn, rare event prediction |\n",
    "| **Oversampling**          | When you want to duplicate minority class samples | Simple and effective                             | Binary classification with high imbalance |\n",
    "| **NearMiss**             \n",
    "\n",
    " | When you want to retain important majority class samples | Focuses on informative majority class samples     | Medical diagnosis, edge cases        |\n",
    "| **Balanced Class Weights**| When you want to avoid dataset modification     | Adjusts model training without altering data      | Any imbalanced classification task    |\n",
    "| **Ensemble Methods**      | When multiple models can boost performance      | Combines resampling and multiple models           | Any highly imbalanced dataset         |\n",
    "\n",
    "---\n",
    "\n",
    "These techniques can be crucial to improving the performance of machine learning models when dealing with imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
